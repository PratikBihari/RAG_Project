{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Pipelines-Data Ingesion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDF files to process\n",
      "\n",
      "Processing: Data_science_Interview_question_2.pdf\n",
      "  ✓ Loaded 32 pages\n",
      "\n",
      "Processing: ANNEX.pdf\n",
      "  ✓ Loaded 1 pages\n",
      "\n",
      "Total documents loaded: 33\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 0, 'page_label': '1', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='with These\\n100 Ques tions\\nData Scientist \\nInterview\\nAce your Next'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 1, 'page_label': '2', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nWhat is the role of a data scientist in an \\norganisation?\\nExplain the difference between supervised and \\nunsupervised learning.\\r\\nWhat is cross-validation, and why is it important?\\r\\nA data scientist is responsible for collecting, analysing, \\nand interpreting complex data to help organisations \\nmake informed decisions.\\nSupervised learning uses labelled data for training, \\nwhile unsupervised learning\\rworks with unlabeled \\ndata to find hidden patterns or relationships.\\r\\nCross-validation is a technique used to assess how \\nwell a model generalises to\\ran independent dataset. It \\nis important for evaluating a model's performance and \\npreventing\\roverfitting.\\r\\nQ.1\\nQ.2\\nQ.3\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 2, 'page_label': '3', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nCan you explain the steps involved in the \\ndata preprocessing process?\\r\\nWhat are some common algorithms used in \\nmachine learning?\\r\\nData preprocessing includes data cleaning, handling \\nmissing values, data\\rtransformation, normalisation, \\nand standardisation to prepare the data for analysis \\nand\\rmodelling.\\r\\nCommon machine learning algorithms include linear \\nregression, logistic\\rregression, decision trees, random \\nforests, support vector machines, and neural networks.\\nQ.4\\nQ.5\\nHow do you handle missing data in a dataset?\\nMissing data can be handled by either removing the \\nrows with missing values,\\rimputing the missing values \\nusing statistical techniques, or using advanced \\nimputation\\rmethods such as K-Nearest Neighbors.\\nQ.6'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 3, 'page_label': '4', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nWhat is the purpose of the K-Means clustering \\nalgorithm?\\r\\nHow do you assess the performance of a machine \\nlearning model?\\r\\nExplain the term 'bias' in the context of machine \\nlearning models.\\r\\nWhat is the importance of feature scaling in \\nmachine learning?\\nThe K-Means algorithm is used for partitioning a \\ndataset into K clusters, aiming to\\rminimise the sum of \\nsquares within each cluster.\\r\\nModel performance can be assessed using metrics \\nsuch as accuracy, precision,\\rrecall, F1 score, and the \\nROC curve for classification tasks, and metrics such as \\nmean\\rsquared error for regression tasks.\\nBias refers to the error introduced by approximating a \\nreal-world problem, often\\rdue to oversimplification of \\nthe model. High bias can result in underfitting.\\r\\n Feature scaling ensures that the features are at a \\nsimilar scale, preventing\\rcertain features from \\ndominating the learning process and helping the \\nalgorithm converge\\rfaster.\\nQ.7\\nQ.8\\nQ.9\\nQ.10\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 4, 'page_label': '5', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nCan you explain the concept of regularisation in \\nmachine learning?\\r\\nWhat is the difference between L1 and L2 \\nregularisation?\\nRegularisation is a technique used to prevent \\noverfitting by adding a penalty term\\rto the loss \\nfunction, discouraging complex models.\\nL1 regularisation adds the absolute value of the \\nmagnitude of coefficients as a\\rpenalty term, while L2 \\nregularisation adds the square of the magnitude of \\ncoefficients as a\\rpenalty term.\\r\\nQ.11\\nQ.12\\nWhat is the purpose of a confusion matrix in \\nclassification tasks?\\r\\nA confusion matrix is used to visualise the performance \\nof a classification model,\\rshowing the counts of true \\npositive, true negative, false positive, and false \\nnegative\\rpredictions.\\nQ.13'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 5, 'page_label': '6', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nHow do you handle multicollinearity in a dataset?\\nCan you explain the difference between precision \\nand recall?\\nWhat is the purpose of the Naive Bayes algorithm \\nin machine learning?\\nMulticollinearity  can be handled by \\ntechniques such as removing one of \\nthe\\rcorrelated features, using principal \\ncomponent analysis, or using \\nregularisation techniques to\\rreduce \\nthe impact of correlated features.\\nPrecision refers to the ratio of correctly predicted \\npositive observations to the\\rtotal predicted positive \\nobservations, while recall refers to the ratio of correctly \\npredicted\\rpositive observations to the total actual \\npositive observations.\\nThe Naive Bayes algorithm is used for classification \\ntasks, based on the Bayes\\rtheorem with the \\nassumption of independence between features.\\nQ.14\\nQ.15\\nQ.16'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 6, 'page_label': '7', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nHow do you handle outliers in a dataset?\\nExplain the concept of the Central Limit Theorem.\\nWhat is the purpose of a decision tree algorithm in \\nmachine learning?\\nCan you explain the concept of ensemble \\nlearning?\\nOutliers can be handled by either removing them if \\nthey are due to data entry\\rerrors, or by transforming \\nthem using techniques such as winsorization or log \\ntransformation.\\nThe Central Limit Theorem states that the sampling \\ndistribution of the sample\\rmeans approaches a normal \\ndistribution as the sample size increases, regardless of \\nthe\\rshape of the population distribution.\\nDecision trees are used for both classification and \\nregression tasks, creating a\\rmodel that predicts the \\nvalue of a target variable by learning simple decision \\nrules inferred\\rfrom the data features.\\nEnsemble learning involves combining multiple \\nindividual models to improve the\\roverall performance \\nand predictive power of the learning algorithm.\\nQ.17\\nQ.18\\nQ.19\\nQ.20'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 7, 'page_label': '8', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the difference between bagging and \\nboosting?\\nExplain the purpose of the Random Forest \\nalgorithm in machine learning.\\nHow do you select the optimal number of clusters \\nin a K-Means clustering algorithm?\\nBagging involves training each model in the ensemble \\nwith a subset of the data,\\rwhile boosting focuses on \\ntraining each model sequentially, giving more weight \\nto the\\rmisclassified data points.\\nRandom Forest is an ensemble learning method that \\nconstructs multiple decision\\rtrees during training and \\noutputs the mode of the classes or the mean \\nprediction of the\\rindividual trees for classification and \\nregression tasks, respectively.\\nThe optimal number of clusters can be determined \\nusing techniques such as the\\relbow method, silhouette \\nscore, or the gap statistic.\\nQ.21\\nQ.22\\nQ.23'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 8, 'page_label': '9', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Support Vector Machine \\n(SVM) algorithm?\\nHow do you handle a large volume of data that \\ncannot fit into memory?\\nCan you explain the purpose of a recommendation \\nsystem?\\nWhat is the purpose of Principal Component \\nAnalysis (PCA) in machine learning?\\nSupport Vector Machines are used for classification \\nand regression analysis, with\\rthe primary goal of \\nfinding the hyperplane that best separates the classes.\\nLarge volumes of data can be handled using \\ntechniques such as data streaming,\\rdistributed \\ncomputing frameworks like Hadoop or Spark, and \\ndata compression techniques.\\nRecommendation systems are used to predict and \\nrecommend items or products\\rthat a user may be \\ninterested in, based on their past preferences or \\nbehaviour.\\nPrincipal Component Analysis is used for dimensionality \\nreduction, transforming\\ra large set of variables into a \\nsmaller set of uncorrelated variables while retaining \\nmost of the\\rinformation.\\nQ.24\\nQ.25\\nQ.26\\nQ.27'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 9, 'page_label': '10', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nHow do you handle a situation where the data is \\ntoo imbalanced?\\nWhat is the purpose of a Recurrent Neural Network \\n(RNN) in deep learning?\\nExplain the concept of a Long Short-Term Memory \\n(LSTM) network.\\nImbalanced data can be handled using techniques \\nsuch as oversampling the\\rminority class, \\nundersampling the majority class, or using algorithms \\nspecifically designed to\\rhandle imbalanced datasets.\\nRecurrent Neural Networks are used for sequence data, \\nallowing information to\\rpersist over time, making them \\nsuitable for tasks such as natural language processing \\nand\\rtime series analysis.\\nLSTM networks are a type of RNN that addresses the \\nvanishing gradient\\rproblem, making them more \\neffective for learning and predicting sequences of data.\\nQ.28\\nQ.29\\nQ.30'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 10, 'page_label': '11', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Word2Vec algorithm in \\nnatural language processing?\\nHow do you handle a situation where there are \\ntoo many features compared to the\\rnumber of \\nobservations?\\nExplain the concept of a support vector in the \\ncontext of a Support Vector Machine\\ralgorithm.\\nWord2Vec is used for learning word embeddings, \\nrepresenting words as vectors\\rto capture semantic \\nrelationships between words in a text corpus.\\nThe situation of having too many features compared \\nto the number of\\robservations can be handled by \\nusing feature selection techniques, such as Lasso\\r\\nregression, or by using dimensionality reduction \\ntechniques like PCA or t-SNE.\\r\\nSupport vectors are data points that lie closest to the \\ndecision boundary between\\rthe classes, influencing the \\nposition and orientation of the hyperplane in a Support \\nVector\\rMachine.\\nQ.31\\nQ.32\\nQ.33'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 11, 'page_label': '12', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Root Mean Square Error \\n(RMSE) metric in regression tasks?\\nCan you explain the purpose of the Apriori \\nalgorithm in association rule mining?\\nHow do you handle a situation where the data is \\nhighly skewed?\\nThe Root Mean Square Error is a commonly used metric \\nfor evaluating the\\raccuracy of a regression model by \\nmeasuring the differences between the predicted \\nvalues\\rand the actual values.\\nThe Apriori algorithm is used for discovering frequent \\nitemsets within a\\rtransactional database and is \\ncommonly employed in market basket analysis to \\nidentify\\rpatterns or relationships between different \\nitems.\\nHighly skewed data can be handled by using \\ntransformations such as log\\rtransformations, square \\nroot transformations, or by using specialised models \\nthat can handle\\rskewed data more effectively.\\nQ.34\\nQ.35\\nQ.36'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 12, 'page_label': '13', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Mean Average Precision \\n(MAP) metric in evaluating\\rinformation retrieval \\nsystems?\\nExplain the purpose of the Euclidean distance \\nmetric in clustering tasks.\\nHow do you handle a situation where the data is \\nnot linearly separable?\\nMean Average Precision is used to evaluate the \\nperformance of information\\rretrieval systems, \\nmeasuring the average precision at each relevant \\ndocument retrieved\\racross multiple queries.\\nThe Euclidean distance metric is used to measure the \\ndistance between two\\rpoints in a multidimensional \\nspace and is commonly used in clustering algorithms \\nsuch as\\rK-Means.\\nIn cases where the data is not linearly separable, kernel \\nfunctions can be used in\\ralgorithms like Support Vector \\nMachines to map the data to a higher-dimensional \\nspace\\rwhere it becomes linearly separable.\\nQ.37\\nQ.38\\nQ.39'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 13, 'page_label': '14', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Chi-square test in \\nfeature selection?\\nCan you explain the purpose of the Gradient \\nDescent algorithm in machine learning?\\nHow do you handle a situation where the data is \\ntime-series data?\\nThe Chi-square test is used to determine the \\nindependence of two categorical\\rvariables, making it \\nsuitable for feature selection in classification tasks.\\nGradient Descent is an optimization algorithm used to \\nminimise the cost function\\rand find the optimal \\nparameters of a model by iteratively updating the \\nparameters in the\\rdirection of the steepest descent.\\nTime-series data can be handled using techniques \\nsuch as autoregressive\\rintegrated moving average \\n(ARIMA) models, exponential smoothing methods, or \\nmore\\radvanced deep learning models like Long Short-\\nTerm Memory (LSTM) networks.\\nQ.40\\nQ.41\\nQ.42'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 14, 'page_label': '15', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the K-Nearest Neighbors \\n(KNN) algorithm in machine learning?\\nExplain the purpose of the Log Loss metric in \\nevaluating classification models.\\nHow do you handle a situation where the data is \\nhigh-dimensional?\\nThe K-Nearest Neighbors algorithm is used for \\nclassification and regression\\rtasks, making predictions \\nbased on the majority vote of its k nearest neighbours.\\nLog Loss is used to evaluate the performance of a \\nclassification model that\\routputs probabilities, \\nmeasuring the performance based on the likelihood of \\nthe predicted\\rprobabilities matching the actual labels.\\nHigh-dimensional data can be handled by using \\ndimensionality reduction\\rtechniques such as Principal \\nComponent Analysis (PCA), t-Distributed Stochastic \\nNeighbour\\rEmbedding (t-SNE), or by employing feature \\nselection methods.\\nQ.43\\nQ.44\\nQ.45'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 15, 'page_label': '16', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the R-squared (R2) metric \\nin evaluating regression models?\\nCan you explain the purpose of the Gini index in \\nthe context of a decision tree\\ralgorithm?\\nHow do you handle a situation where there is noise \\nin the data?\\r\\nR-squared is a statistical measure that represents the \\nproportion of the variance\\rfor a dependent variable \\nthat is explained by an independent variable in a \\nregression model.\\nThe Gini index is used to measure the impurity or the \\nhomogeneity of a node in a\\rdecision tree, helping to \\ndetermine the best split for creating a more accurate \\ndecision tree.\\r\\nNoise in the data can be handled by smoothing \\ntechniques such as moving\\raverages, using robust \\nstatistics, or employing filtering methods to remove \\noutliers and\\rirrelevant data points.\\nQ.46\\nQ.47\\nQ.48'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 16, 'page_label': '17', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nWhat is the purpose of the F1 score metric in \\nevaluating classification models?\\nWhat is the difference between classification and \\nregression in machine learning?\\nCan you explain the bias-variance trade-off in the \\ncontext of model complexity?\\nCan you explain the purpose of the LDA \\n(Linear Discriminant Analysis) algorithm in\\r\\nmachine learning?\\nThe F1 score is the harmonic mean of precision and \\nrecall and is used to\\revaluate the balance between \\nprecision and recall in a classification model.\\nClassification is used to predict discrete categories, \\nwhile regression is used to\\rpredict continuous \\nquantities.\\r\\nThe bias-variance trade-off highlights the trade-off \\nbetween a model's ability to\\rminimise errors due to \\nbias and variance. Increasing model complexity \\nreduces bias but\\rincreases variance and vice versa.\\nLinear Discriminant Analysis is used for dimensionality \\nreduction and\\rclassification tasks, aiming to find the \\nlinear combinations of features that best separate\\r\\nmultiple classes in the data.\\nQ.49\\nQ.51\\nQ.52\\nQ.50\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 17, 'page_label': '18', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nHow do you handle imbalanced data sets when \\nbuilding a classification model?\\nExplain the purpose of the term 'regularisation' in \\nmachine learning models.\\nHow do you assess the performance of a \\nclassification model apart from accuracy?\\nWhat is the purpose of the term 'gradient descent' \\nin the context of optimising a model?\\nImbalanced datasets can be handled using techniques \\nlike oversampling,\\rundersampling, or using algorithms \\ndesigned for imbalanced data such as SMOTE\\r\\n(Synthetic Minority Over-sampling Technique).\\nRegularisation is a technique used to prevent \\noverfitting by adding a penalty term\\rto the loss \\nfunction, discouraging overly complex models.\\nThe performance of a classification model can be \\nevaluated using metrics such as\\rprecision, recall, F1 \\nscore, and the area under the ROC curve.\\nGradient descent is an iterative optimization algorithm \\nused to minimise the cost\\rfunction of a model by \\nadjusting the model's parameters in the direction of \\nsteepest descent.\\r\\nQ.53\\nQ.54\\nQ.56\\nQ.55\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 18, 'page_label': '19', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nCan you explain the concept of 'feature selection' \\nand its importance in model building?\\nWhat is the purpose of the term 'cross-validation' \\nin model training and evaluation?\\nHow do you handle missing data in a dataset while \\nbuilding a predictive model?\\nFeature selection involves selecting the most relevant \\nfeatures from a dataset. It is\\rcrucial for improving \\nmodel performance, reducing overfitting, and \\nenhancing interpretability.\\nCross-validation is used to assess how well a model \\ngeneralises to an\\rindependent dataset, minimising the \\nrisk of overfitting and providing a more accurate\\r\\nestimate of the model's performance.\\nMissing data can be handled by \\ntechniques such as mean/median \\nimputation,\\rmode imputation, or \\nusing advanced methods like \\nmultiple imputation or K-Nearest\\r\\nNeighbors imputation.\\nQ.57\\nQ.58\\nQ.59\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 19, 'page_label': '20', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nExplain the purpose of the term 'ensemble learning' \\nand its benefits in model building.\\nWhat is the difference between unsupervised and \\nsupervised machine learning\\ralgorithms?\\nCan you explain the concept of 'clustering' and \\nprovide an example of when it is used?\\nWhat is the purpose of 'dimensionality reduction' in \\ndata analysis, and how is it\\rachieved?\\nEnsemble learning involves combining multiple models \\nto improve predictive\\rperformance and reduce \\noverfitting, often resulting in better generalisation and \\nmore robust\\rpredictions.\\r\\nSupervised learning uses labelled data for training, \\nwhile unsupervised learning\\rworks with unlabeled data \\nto find patterns and relationships.\\nClustering is an unsupervised learning technique used \\nto group similar data points\\rtogether. An example is \\ncustomer segmentation in marketing.\\nDimensionality reduction is used to reduce the number \\nof features in a dataset. It is\\rachieved through \\ntechniques like principal component analysis (PCA) \\nand t-distributed\\rstochastic neighbour embedding (t-\\nSNE).\\r\\nQ.60\\nQ.61\\nQ.62\\nQ.63\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 20, 'page_label': '21', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nHow do you handle the problem of overfitting in \\nmachine learning models?\\nHow do you handle the problem of multicollinearity \\nin a dataset?\\nExplain the purpose of the term 'Naive Bayes' in \\nmachine learning and its application.\\nWhat is the purpose of the term 'decision trees' in \\nmachine learning, and how does it\\rwork?\\nOverfitting can be mitigated by using techniques like \\ncross-validation,\\rregularisation, early stopping, and \\nreducing model complexity.\\nMulticollinearity can be addressed by techniques such \\nas removing one of the\\rcorrelated features, using \\nprincipal component analysis (PCA), or using \\nregularisation\\rmethods.\\nNaive Bayes is a probabilistic classification algorithm \\nbased on Bayes' theorem\\rwith an assumption of \\nindependence between features. It is commonly used \\nfor text\\rclassification and spam filtering.\\nDecision trees are predictive models that map features \\nto conclusions about the\\rtarget value. They work by \\nsplitting the dataset into smaller subsets based on the \\nmost\\rsignificant differentiators in the data.\\nQ.64\\nQ.67\\nQ.65\\nQ.66\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 21, 'page_label': '22', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nCan you explain the purpose of the term 'random \\nforest' in machine learning and its\\radvantages?\\nWhat is the purpose of 'data preprocessing' in \\nmachine learning, and what are some\\rcommon \\ntechniques used?\\nHow do you handle the problem of underfitting in a \\nmachine learning model?\\nRandom forests are an ensemble learning method that \\nconstructs multiple decision\\rtrees during training. They \\nare effective for reducing overfitting and handling large \\ndatasets\\rwith high dimensionality.\\nData preprocessing involves preparing and cleaning \\ndata before it is fed into a\\rmachine learning model. \\nCommon techniques include data normalisation, \\nstandardisation,\\rand handling missing values.\\nUnderfitting can be addressed by using more complex \\nmodels, adding more\\rfeatures, or reducing \\nregularisation, allowing the model to capture more \\ncomplex patterns in\\rthe data.\\nQ.68\\nQ.69\\nQ.70\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 22, 'page_label': '23', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'hyperparameter tuning' in \\nmachine learning algorithms.\\nWhat is the purpose of 'ANOVA' (Analysis of \\nVariance) in statistical analysis, and when is\\rit used?\\nHow do you handle a situation where the data has \\noutliers?\\nExplain the concept of 'bias' in machine learning \\nmodels.\\nHyperparameter tuning involves finding the best set of \\nhyperparameters for a\\rmachine learning model to \\noptimise its performance and generalisation.\\nANOVA is used to analyse the differences among group \\nmeans and is applied\\rwhen comparing means of more \\nthan two groups to determine whether they are \\nstatistically\\rsignificantly different.\\nOutliers can be handled by removing them if they are \\ndue to data entry errors or by\\rtransforming them using \\ntechniques such as winsorization or log transformation.\\nBias refers to the error introduced by approximating a \\nreal-world problem, often\\rdue to oversimplification of \\nthe model. High bias can lead to underfitting.\\nQ.71\\nQ.72\\nQ.73\\nQ.74\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 23, 'page_label': '24', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nWhat is the purpose of the 'mean squared error' \\nmetric in regression analysis?\\nCan you explain the purpose of the term 'cosine \\nsimilarity' in similarity measurements?\\nHow do you handle a situation where the data has \\na time component?\\nMean squared error is a commonly used metric for \\nevaluating the performance of a\\rregression model by \\nmeasuring the average of the squares of the \\ndifferences between\\rpredicted and actual values.\\nCosine similarity is a metric used to measure the \\nsimilarity between two non-zero\\rvectors, often used in \\ntext mining and collaborative filtering.\\nData with a time component can be analysed using \\ntime series analysis techniques\\rsuch as\\rautoregressive \\nintegrated moving average (ARIMA) models, \\nexponential smoothing, or\\rProphet forecasting models.\\r\\nQ.75\\nQ.76\\nQ.77\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 24, 'page_label': '25', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'precision' and 'recall' in the \\ncontext of classification models.\\nWhat is the purpose of the 'Hadoop' framework in \\nbig data processing, and how is it\\rused?\\nHow do you handle a situation where the data has \\na lot of noise?\\nPrecision measures the proportion of true positive \\nresults among the predicted\\rpositive results, while \\nrecall measures the proportion of true positive results \\namong the\\ractual positive results.\\nHadoop is an open-source framework used for \\ndistributed storage and processing\\rof large data sets \\nacross clusters of computers using simple \\nprogramming models.\\nNoisy data can be managed through techniques such \\nas data smoothing, filtering,\\ror by using robust \\nstatistical measures that are less sensitive to outliers.\\nQ.78\\nQ.79\\nQ.80\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 25, 'page_label': '26', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'correlation' in statistics and \\nits different types.\\nWhat is the purpose of the 'k-nearest neighbours' \\nalgorithm in machine learning, and\\rhow does it \\nwork?\\nHow do you handle a situation where the data has \\na lot of categorical variables?\\nCorrelation measures the relationship between two \\nvariables and can be positive,\\rnegative, or zero, \\nindicating the strength and direction of the \\nrelationship.\\r\\nThe k-nearest neighbours algorithm is used for \\nclassification and regression tasks,\\rmaking predictions \\nbased on the majority vote or averaging the values of \\nthe k nearest\\rneighbours.\\nCategorical variables can be handled through \\ntechniques such as one-hot\\rencoding, label encoding, \\nor using target encoding to convert them into a format \\nsuitable for\\rmachine learning models.\\nQ.81\\nQ.82\\nQ.83\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 26, 'page_label': '27', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"t u t o r t  a c a d e m yC u r a t e d  b y\\nExplain the purpose of the 'SVM' (Support Vector \\nMachine) algorithm in machine\\rlearning, and its \\nadvantages.\\nSupport Vector Machines are supervised learning \\nmodels used for classification\\rand regression analysis. \\nThey are effective in high-dimensional spaces and \\nwork well with\\rcomplex datasets.\\nQ.84\\nWhat is the purpose of the 'LSTM' \\n(Long Short-Term Memory) network in deep \\nlearning,\\rand how is it used?\\nCan you explain the purpose of the term 'Principal \\nComponent Analysis' (PCA)\\rin dimensionality \\nreduction, and how is it used?\\nLSTM networks are a type of recurrent neural network \\n(RNN) used for processing\\rand making predictions \\nbased on sequential data, often used in natural \\nlanguage processing\\rand time series analysis.\\nPrincipal Component Analysis is a technique used to \\nreduce the dimensionality of a\\rdataset while \\npreserving as much variance as possible. It transforms \\nthe original variables\\rinto a new set of variables, the \\nprincipal components, which are orthogonal and\\r\\nuncorrelated. This aids in simplifying the dataset and \\nspeeding up the subsequent learning\\ralgorithms while \\nretaining most of the essential information.\\nQ.85\\nQ.86\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 27, 'page_label': '28', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'k-means clustering' and its \\napplication in unsupervised learning.\\nWhat is the purpose of the 'R-squared' metric in \\nregression analysis, and what does it\\rindicate \\nabout the model's fit?\\nWhat is the purpose of the term 't-Distributed \\nStochastic Neighbour Embedding' (t-SNE)\\r\\nin dimensionality reduction, and how is it used?\\nK-means clustering is a popular unsupervised learning \\nalgorithm used for\\rpartitioning a dataset into K clusters \\nbased on similarities in the data points.\\nR-squared is a statistical measure that represents the \\nproportion of the variance for\\ra dependent variable \\nexplained by the independent variables in a regression \\nmodel. It\\rindicates the goodness of fit of the model.\\nt-Distributed Stochastic Neighbour Embedding is a \\nnonlinear dimensionality\\rreduction technique used for \\nvisualising high-dimensional data in a low-\\ndimensional space. It\\ris particularly useful for \\nvisualising complex datasets and identifying patterns \\nor clusters\\rwithin the data.\\nQ.87\\nQ.88\\nQ.89\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 28, 'page_label': '29', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nExplain the purpose of the 'F1 score' metric in \\nevaluating classification models and its\\r\\nrelationship with precision and recall.\\nCan you explain the concept of 'backpropagation' \\nin neural networks and its role in\\rtraining the \\nmodel?\\nThe F1 score is the harmonic mean of precision and \\nrecall and is used to evaluate\\rthe balance between \\nprecision and recall in a classification model.\\r\\nBackpropagation is an algorithm used to train artificial \\nneural networks by adjusting\\rthe weights of the \\nconnections in the network to minimise the difference \\nbetween predicted\\rand actual outputs.\\nQ.90\\nQ.91\\nWhat is the purpose of the 'chi-square test' in \\nstatistics, and when is it used?\\nThe chi-square test is used to determine the \\nindependence of two categorical\\rvariables and is often \\nused to test the significance of relationships between \\nvariables in a\\rcontingency table.\\r\\nQ.92\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 29, 'page_label': '30', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nHow do you handle a situation where the data is \\nnot normally distributed?\\nExplain the concept of 'latent variables' in the \\ncontext of factor analysis and its\\rimportance.\\nWhat is the purpose of the 'Gini index' in decision \\ntrees, and how is it used in the context\\rof building \\nthe tree?\\nNon-normally distributed data can be transformed \\nusing techniques such as the\\rBox-Cox transformation, \\nYeo-Johnson transformation, or log transformation to \\napproximate a\\rnormal distribution.\\nLatent variables are variables that are not directly \\nobserved but are inferred from\\robserved variables. \\nThey are crucial for capturing underlying factors and \\nreducing the\\rdimensionality of the data.\\nThe Gini index is a metric used to measure the impurity \\nof a node in a decision\\rtree. It is used to find the best \\nsplit for creating a more accurate decision tree.\\nQ.93\\nQ.94\\nQ.95\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 30, 'page_label': '31', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"t u t o r t  a c a d e m yC u r a t e d  b y\\nHow do you handle a situation where the data has \\na lot of continuous variables?\\nExplain the purpose of 'association rules' in data \\nmining, and provide an example of its\\rapplication.\\nWhat is the purpose of the 'logistic function' in \\nlogistic regression, and how is it used for\\r\\nbinary classification?\\nContinuous variables can be handled through \\ntechniques such as scaling and\\rnormalisation to \\nensure that the variables are on a similar scale, \\npreventing certain features\\rfrom dominating the \\nlearning process.\\nAssociation rules are used to discover interesting \\nrelationships between variables\\rin large datasets. An \\nexample is market basket analysis used to identify \\nproducts frequently\\rpurchased together.\\nThe logistic function is used to model the probability of \\na binary outcome. It maps\\rany real-valued number to \\na value between 0 and 1, making it suitable for binary\\r\\nclassification tasks.\\nQ.96\\nQ.97\\nQ.98\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 31, 'page_label': '32', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nHow do you handle a situation where the data has \\na lot of missing values?\\nExplain the concept of 'bagging' and 'boosting' in \\nensemble learning, and provide an\\r\\nexample of when each technique is used.\\nData with missing values can be managed through \\ntechniques such as imputation,\\rusing algorithms like \\nK-Nearest Neighbours, decision trees, or employing \\nadvanced\\rtechniques like deep learning-based \\nimputation.\\nBagging involves training multiple models \\nindependently and combining their\\rpredictions, while \\nboosting trains models sequentially, giving more \\nweight to misclassified\\rdata points. Bagging is used for \\nreducing variance, while boosting is used for reducing \\nbias\\rin ensemble models.\\nQ.99\\nQ.100\\nHighest \\nCTC\\nHiring\\nPartners350+Career \\nTransitions1250+ 2.1CR\\nWhy Tutort Academy?\"),\n",
       " Document(metadata={'producer': 'iText 2.1.3 (by lowagie.com)', 'creator': 'PyPDF', 'creationdate': '2026-02-02T21:28:22+05:30', 'moddate': '2026-02-02T21:28:22+05:30', 'source': '../data/pdf/ANNEX.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'ANNEX.pdf', 'file_type': 'pdf'}, page_content=\"INCOME TAX PAN SERVICES UNIT\\n Managed by Protean eGov Technologies Limited (formerly NSDL e-Governance Infrastructure Limited) \\n4th floor, Sapphire Chambers, Baner Road, Baner, Pune 411045.\\n \\nRef.No.TIN/PAN/CR-/99005971236288402 Date: 19-JAN-2026\\nShri BALMIKI BIHARI \\nC/O-BHIMASEN BIHARI BARIMUNDEI\\nBAGALSAHI NIALI\\nCUTTACK Orissa 754004\\nDear Sir/Madam,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t Subject  : Discrepancy in the documents received for PAN application\\n1. This has reference to documents received in connection with your request for New PAN card and/or changes/correction in PAN data for PAN AJOPB9797Q made vide\\nacknowledgement no. 990059712362884. Following discrepancies are observed between details provided in application and details available with Income Tax Department\\n(ITD).\\n \\n2. Please submit the below specified documents (with details as per application) to clear the above mentioned discrepancy (ies).\\n \\n3. Please note your PAN application will be processed only on receipt of documents as explained above.\\n4. If we do not receive documents as mentioned above within 30 days, then your application will be filed and no further action will be taken.\\n5. Information relating to all PAN Services of ITD can be obtained by making a phone call to Aaykar Sampark Kendra (1800-180-1961) or TIN-Call Centre (020-27218080)\\nor from the website: www.incometaxindia.gov.in or https://www.protean-tinpan.com/index.html \\n(This being a computer-generated letter,no signature is required) Income Tax Department\\nCaution : Income Tax Department does not send e-mails regarding refunds and does not seek any taxpayer information like user name, password, details of ATM, bank accounts,\\ncredit cards, etc. Taxpayers are advised not to part with such information on the basis of emails.\\n \\nTo be sent to Protean along with documents RETURN-SLIP         \\nParticular As per Application As per ITD's Database Discrepancy in documents submitted\\nApplicant's Name BALMIKI  BIHARI BALKI  BIHARI Name change from BALKI  BIHARI to BALMIKI  BIHARI has not been\\nspecified in AADHAR Card issued by the Unique Identification\\nAuthority of India provided by you (Please refer list  1)\\nFather's Name BHIMASEN  BIHARI BHIM SENA  BIHARI Name change from BALKI  BIHARI to BALMIKI  BIHARI has not been\\nspecified in AADHAR Card issued by the Unique Identification\\nAuthority of India provided by you (Please refer list  2)\\nName\\n( any one of the following - List 1 )\\nMarriage Certificate or Marriage invitation Card or\\nPublication of name change in official gazatte or copy of\\npassport showing husband's name or certificate issued by\\ngazetted officer along with copy of office Identity proof of\\ngazetted officer.\\nPublication of name change in official gazatte or certificate\\nissued by a gazetted officer along with copy of office\\nidentity proof of gazetted officer.\\nFather's Name\\n( any one of the following - List 2 )\\nPassport\\nElector's photo identity card\\nDriving License\\nCertificate of identity signed by a Member of Parliament or\\na Member of Legislative Assembly or a Muncipal councilor\\nor a Gazetted Officer along with copy of office identity\\nproof of issuing officer.\\nAny of the below mentioned document in the name of\\nKarta of HUF\\nAadhaar Card issued by the Unique Identification\\nAuthority of India\\nArm's license\\nPensioner card having photograph of the applicant\\nCentral Government Health Service Scheme Card\\nBank certificate in Original on letter head from the branch\\n(alongwith name and stamp of the issuing officer)\\ncontaining duly attested photograph and bank account\\nnumber of the applicant\\nEx-Servicemen Contributory Health Scheme photo card\\nPhoto identity card issued by the Central Government or\\nState Government or Public Sector Undertaking\\n ACKNOWLEDGEMENT NO.990059712362884\\nPlease indicate how you want your application to be processed by putting tick in  appropriate boxes.\\nA.  Reprint PAN card with ITD data, no change required: [ ]\\nB.  For following fields, details available with Income Tax Department is / are correct and should not be changed (ignore application data):\\n      [ ] Name            [ ] Father's Name          [ ] DOB\\nC.  For following fields, details available with Income Tax Department is / are incorrect and should be changed:\\n      [ ] Name            [ ] Father's Name            [ ] DOB          \\t\\t \\t\\t (provide documents to support changes as described overleaf)\\nList of Documents attached: (1)_____________(2)___________(3)___________(4)____________\\nName of Applicant: Shri BALMIKI BIHARI\\nSignature of Applicant: _______________\")]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text Splitting get into chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 33 documents into 45 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: with These\n",
      "100 Ques tions\n",
      "Data Scientist \n",
      "Interview\n",
      "Ace your Next...\n",
      "Metadata: {'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 0, 'page_label': '1', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 0, 'page_label': '1', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='with These\\n100 Ques tions\\nData Scientist \\nInterview\\nAce your Next'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 1, 'page_label': '2', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nWhat is the role of a data scientist in an \\norganisation?\\nExplain the difference between supervised and \\nunsupervised learning.\\r\\nWhat is cross-validation, and why is it important?\\r\\nA data scientist is responsible for collecting, analysing, \\nand interpreting complex data to help organisations \\nmake informed decisions.\\nSupervised learning uses labelled data for training, \\nwhile unsupervised learning\\rworks with unlabeled \\ndata to find hidden patterns or relationships.\\r\\nCross-validation is a technique used to assess how \\nwell a model generalises to\\ran independent dataset. It \\nis important for evaluating a model's performance and \\npreventing\\roverfitting.\\r\\nQ.1\\nQ.2\\nQ.3\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 2, 'page_label': '3', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nCan you explain the steps involved in the \\ndata preprocessing process?\\r\\nWhat are some common algorithms used in \\nmachine learning?\\r\\nData preprocessing includes data cleaning, handling \\nmissing values, data\\rtransformation, normalisation, \\nand standardisation to prepare the data for analysis \\nand\\rmodelling.\\r\\nCommon machine learning algorithms include linear \\nregression, logistic\\rregression, decision trees, random \\nforests, support vector machines, and neural networks.\\nQ.4\\nQ.5\\nHow do you handle missing data in a dataset?\\nMissing data can be handled by either removing the \\nrows with missing values,\\rimputing the missing values \\nusing statistical techniques, or using advanced \\nimputation\\rmethods such as K-Nearest Neighbors.\\nQ.6'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 3, 'page_label': '4', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nWhat is the purpose of the K-Means clustering \\nalgorithm?\\r\\nHow do you assess the performance of a machine \\nlearning model?\\r\\nExplain the term 'bias' in the context of machine \\nlearning models.\\r\\nWhat is the importance of feature scaling in \\nmachine learning?\\nThe K-Means algorithm is used for partitioning a \\ndataset into K clusters, aiming to\\rminimise the sum of \\nsquares within each cluster.\\r\\nModel performance can be assessed using metrics \\nsuch as accuracy, precision,\\rrecall, F1 score, and the \\nROC curve for classification tasks, and metrics such as \\nmean\\rsquared error for regression tasks.\\nBias refers to the error introduced by approximating a \\nreal-world problem, often\\rdue to oversimplification of \\nthe model. High bias can result in underfitting.\\r\\n Feature scaling ensures that the features are at a \\nsimilar scale, preventing\\rcertain features from \\ndominating the learning process and helping the \\nalgorithm converge\\rfaster.\\nQ.7\\nQ.8\\nQ.9\\nQ.10\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 4, 'page_label': '5', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nCan you explain the concept of regularisation in \\nmachine learning?\\r\\nWhat is the difference between L1 and L2 \\nregularisation?\\nRegularisation is a technique used to prevent \\noverfitting by adding a penalty term\\rto the loss \\nfunction, discouraging complex models.\\nL1 regularisation adds the absolute value of the \\nmagnitude of coefficients as a\\rpenalty term, while L2 \\nregularisation adds the square of the magnitude of \\ncoefficients as a\\rpenalty term.\\r\\nQ.11\\nQ.12\\nWhat is the purpose of a confusion matrix in \\nclassification tasks?\\r\\nA confusion matrix is used to visualise the performance \\nof a classification model,\\rshowing the counts of true \\npositive, true negative, false positive, and false \\nnegative\\rpredictions.\\nQ.13'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 5, 'page_label': '6', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nHow do you handle multicollinearity in a dataset?\\nCan you explain the difference between precision \\nand recall?\\nWhat is the purpose of the Naive Bayes algorithm \\nin machine learning?\\nMulticollinearity  can be handled by \\ntechniques such as removing one of \\nthe\\rcorrelated features, using principal \\ncomponent analysis, or using \\nregularisation techniques to\\rreduce \\nthe impact of correlated features.\\nPrecision refers to the ratio of correctly predicted \\npositive observations to the\\rtotal predicted positive \\nobservations, while recall refers to the ratio of correctly \\npredicted\\rpositive observations to the total actual \\npositive observations.\\nThe Naive Bayes algorithm is used for classification \\ntasks, based on the Bayes\\rtheorem with the \\nassumption of independence between features.\\nQ.14\\nQ.15\\nQ.16'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 6, 'page_label': '7', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nHow do you handle outliers in a dataset?\\nExplain the concept of the Central Limit Theorem.\\nWhat is the purpose of a decision tree algorithm in \\nmachine learning?\\nCan you explain the concept of ensemble \\nlearning?\\nOutliers can be handled by either removing them if \\nthey are due to data entry\\rerrors, or by transforming \\nthem using techniques such as winsorization or log \\ntransformation.\\nThe Central Limit Theorem states that the sampling \\ndistribution of the sample\\rmeans approaches a normal \\ndistribution as the sample size increases, regardless of \\nthe\\rshape of the population distribution.\\nDecision trees are used for both classification and \\nregression tasks, creating a\\rmodel that predicts the \\nvalue of a target variable by learning simple decision \\nrules inferred\\rfrom the data features.\\nEnsemble learning involves combining multiple \\nindividual models to improve the\\roverall performance \\nand predictive power of the learning algorithm.\\nQ.17\\nQ.18\\nQ.19\\nQ.20'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 7, 'page_label': '8', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the difference between bagging and \\nboosting?\\nExplain the purpose of the Random Forest \\nalgorithm in machine learning.\\nHow do you select the optimal number of clusters \\nin a K-Means clustering algorithm?\\nBagging involves training each model in the ensemble \\nwith a subset of the data,\\rwhile boosting focuses on \\ntraining each model sequentially, giving more weight \\nto the\\rmisclassified data points.\\nRandom Forest is an ensemble learning method that \\nconstructs multiple decision\\rtrees during training and \\noutputs the mode of the classes or the mean \\nprediction of the\\rindividual trees for classification and \\nregression tasks, respectively.\\nThe optimal number of clusters can be determined \\nusing techniques such as the\\relbow method, silhouette \\nscore, or the gap statistic.\\nQ.21\\nQ.22\\nQ.23'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 8, 'page_label': '9', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Support Vector Machine \\n(SVM) algorithm?\\nHow do you handle a large volume of data that \\ncannot fit into memory?\\nCan you explain the purpose of a recommendation \\nsystem?\\nWhat is the purpose of Principal Component \\nAnalysis (PCA) in machine learning?\\nSupport Vector Machines are used for classification \\nand regression analysis, with\\rthe primary goal of \\nfinding the hyperplane that best separates the classes.\\nLarge volumes of data can be handled using \\ntechniques such as data streaming,\\rdistributed \\ncomputing frameworks like Hadoop or Spark, and \\ndata compression techniques.\\nRecommendation systems are used to predict and \\nrecommend items or products\\rthat a user may be \\ninterested in, based on their past preferences or \\nbehaviour.\\nPrincipal Component Analysis is used for dimensionality \\nreduction, transforming\\ra large set of variables into a \\nsmaller set of uncorrelated variables while retaining \\nmost of the\\rinformation.\\nQ.24\\nQ.25\\nQ.26\\nQ.27'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 9, 'page_label': '10', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nHow do you handle a situation where the data is \\ntoo imbalanced?\\nWhat is the purpose of a Recurrent Neural Network \\n(RNN) in deep learning?\\nExplain the concept of a Long Short-Term Memory \\n(LSTM) network.\\nImbalanced data can be handled using techniques \\nsuch as oversampling the\\rminority class, \\nundersampling the majority class, or using algorithms \\nspecifically designed to\\rhandle imbalanced datasets.\\nRecurrent Neural Networks are used for sequence data, \\nallowing information to\\rpersist over time, making them \\nsuitable for tasks such as natural language processing \\nand\\rtime series analysis.\\nLSTM networks are a type of RNN that addresses the \\nvanishing gradient\\rproblem, making them more \\neffective for learning and predicting sequences of data.\\nQ.28\\nQ.29\\nQ.30'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 10, 'page_label': '11', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Word2Vec algorithm in \\nnatural language processing?\\nHow do you handle a situation where there are \\ntoo many features compared to the\\rnumber of \\nobservations?\\nExplain the concept of a support vector in the \\ncontext of a Support Vector Machine\\ralgorithm.\\nWord2Vec is used for learning word embeddings, \\nrepresenting words as vectors\\rto capture semantic \\nrelationships between words in a text corpus.\\nThe situation of having too many features compared \\nto the number of\\robservations can be handled by \\nusing feature selection techniques, such as Lasso\\r\\nregression, or by using dimensionality reduction \\ntechniques like PCA or t-SNE.\\r\\nSupport vectors are data points that lie closest to the \\ndecision boundary between\\rthe classes, influencing the \\nposition and orientation of the hyperplane in a Support \\nVector\\rMachine.\\nQ.31\\nQ.32\\nQ.33'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 11, 'page_label': '12', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Root Mean Square Error \\n(RMSE) metric in regression tasks?\\nCan you explain the purpose of the Apriori \\nalgorithm in association rule mining?\\nHow do you handle a situation where the data is \\nhighly skewed?\\nThe Root Mean Square Error is a commonly used metric \\nfor evaluating the\\raccuracy of a regression model by \\nmeasuring the differences between the predicted \\nvalues\\rand the actual values.\\nThe Apriori algorithm is used for discovering frequent \\nitemsets within a\\rtransactional database and is \\ncommonly employed in market basket analysis to \\nidentify\\rpatterns or relationships between different \\nitems.\\nHighly skewed data can be handled by using \\ntransformations such as log\\rtransformations, square \\nroot transformations, or by using specialised models \\nthat can handle\\rskewed data more effectively.\\nQ.34\\nQ.35\\nQ.36'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 12, 'page_label': '13', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Mean Average Precision \\n(MAP) metric in evaluating\\rinformation retrieval \\nsystems?\\nExplain the purpose of the Euclidean distance \\nmetric in clustering tasks.\\nHow do you handle a situation where the data is \\nnot linearly separable?\\nMean Average Precision is used to evaluate the \\nperformance of information\\rretrieval systems, \\nmeasuring the average precision at each relevant \\ndocument retrieved\\racross multiple queries.\\nThe Euclidean distance metric is used to measure the \\ndistance between two\\rpoints in a multidimensional \\nspace and is commonly used in clustering algorithms \\nsuch as\\rK-Means.\\nIn cases where the data is not linearly separable, kernel \\nfunctions can be used in\\ralgorithms like Support Vector \\nMachines to map the data to a higher-dimensional \\nspace\\rwhere it becomes linearly separable.\\nQ.37\\nQ.38\\nQ.39'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 13, 'page_label': '14', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Chi-square test in \\nfeature selection?\\nCan you explain the purpose of the Gradient \\nDescent algorithm in machine learning?\\nHow do you handle a situation where the data is \\ntime-series data?\\nThe Chi-square test is used to determine the \\nindependence of two categorical\\rvariables, making it \\nsuitable for feature selection in classification tasks.\\nGradient Descent is an optimization algorithm used to \\nminimise the cost function\\rand find the optimal \\nparameters of a model by iteratively updating the \\nparameters in the\\rdirection of the steepest descent.\\nTime-series data can be handled using techniques \\nsuch as autoregressive\\rintegrated moving average \\n(ARIMA) models, exponential smoothing methods, or \\nmore\\radvanced deep learning models like Long Short-\\nTerm Memory (LSTM) networks.\\nQ.40\\nQ.41\\nQ.42'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 14, 'page_label': '15', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the K-Nearest Neighbors \\n(KNN) algorithm in machine learning?\\nExplain the purpose of the Log Loss metric in \\nevaluating classification models.\\nHow do you handle a situation where the data is \\nhigh-dimensional?\\nThe K-Nearest Neighbors algorithm is used for \\nclassification and regression\\rtasks, making predictions \\nbased on the majority vote of its k nearest neighbours.\\nLog Loss is used to evaluate the performance of a \\nclassification model that\\routputs probabilities, \\nmeasuring the performance based on the likelihood of \\nthe predicted\\rprobabilities matching the actual labels.\\nHigh-dimensional data can be handled by using \\ndimensionality reduction\\rtechniques such as Principal \\nComponent Analysis (PCA), t-Distributed Stochastic \\nNeighbour\\rEmbedding (t-SNE), or by employing feature \\nselection methods.\\nQ.43\\nQ.44\\nQ.45'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 15, 'page_label': '16', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the R-squared (R2) metric \\nin evaluating regression models?\\nCan you explain the purpose of the Gini index in \\nthe context of a decision tree\\ralgorithm?\\nHow do you handle a situation where there is noise \\nin the data?\\r\\nR-squared is a statistical measure that represents the \\nproportion of the variance\\rfor a dependent variable \\nthat is explained by an independent variable in a \\nregression model.\\nThe Gini index is used to measure the impurity or the \\nhomogeneity of a node in a\\rdecision tree, helping to \\ndetermine the best split for creating a more accurate \\ndecision tree.\\r\\nNoise in the data can be handled by smoothing \\ntechniques such as moving\\raverages, using robust \\nstatistics, or employing filtering methods to remove \\noutliers and\\rirrelevant data points.\\nQ.46\\nQ.47\\nQ.48'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 16, 'page_label': '17', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nWhat is the purpose of the F1 score metric in \\nevaluating classification models?\\nWhat is the difference between classification and \\nregression in machine learning?\\nCan you explain the bias-variance trade-off in the \\ncontext of model complexity?\\nCan you explain the purpose of the LDA \\n(Linear Discriminant Analysis) algorithm in\\r\\nmachine learning?\\nThe F1 score is the harmonic mean of precision and \\nrecall and is used to\\revaluate the balance between \\nprecision and recall in a classification model.\\nClassification is used to predict discrete categories, \\nwhile regression is used to\\rpredict continuous \\nquantities.\\r\\nThe bias-variance trade-off highlights the trade-off \\nbetween a model's ability to\\rminimise errors due to \\nbias and variance. Increasing model complexity \\nreduces bias but\\rincreases variance and vice versa.\\nLinear Discriminant Analysis is used for dimensionality \\nreduction and\\rclassification tasks, aiming to find the\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 16, 'page_label': '17', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='reduces bias but\\rincreases variance and vice versa.\\nLinear Discriminant Analysis is used for dimensionality \\nreduction and\\rclassification tasks, aiming to find the \\nlinear combinations of features that best separate\\r\\nmultiple classes in the data.\\nQ.49\\nQ.51\\nQ.52\\nQ.50'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 17, 'page_label': '18', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nHow do you handle imbalanced data sets when \\nbuilding a classification model?\\nExplain the purpose of the term 'regularisation' in \\nmachine learning models.\\nHow do you assess the performance of a \\nclassification model apart from accuracy?\\nWhat is the purpose of the term 'gradient descent' \\nin the context of optimising a model?\\nImbalanced datasets can be handled using techniques \\nlike oversampling,\\rundersampling, or using algorithms \\ndesigned for imbalanced data such as SMOTE\\r\\n(Synthetic Minority Over-sampling Technique).\\nRegularisation is a technique used to prevent \\noverfitting by adding a penalty term\\rto the loss \\nfunction, discouraging overly complex models.\\nThe performance of a classification model can be \\nevaluated using metrics such as\\rprecision, recall, F1 \\nscore, and the area under the ROC curve.\\nGradient descent is an iterative optimization algorithm \\nused to minimise the cost\\rfunction of a model by \\nadjusting the model's parameters in the direction of\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 17, 'page_label': '18', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"Gradient descent is an iterative optimization algorithm \\nused to minimise the cost\\rfunction of a model by \\nadjusting the model's parameters in the direction of \\nsteepest descent.\\r\\nQ.53\\nQ.54\\nQ.56\\nQ.55\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 18, 'page_label': '19', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nCan you explain the concept of 'feature selection' \\nand its importance in model building?\\nWhat is the purpose of the term 'cross-validation' \\nin model training and evaluation?\\nHow do you handle missing data in a dataset while \\nbuilding a predictive model?\\nFeature selection involves selecting the most relevant \\nfeatures from a dataset. It is\\rcrucial for improving \\nmodel performance, reducing overfitting, and \\nenhancing interpretability.\\nCross-validation is used to assess how well a model \\ngeneralises to an\\rindependent dataset, minimising the \\nrisk of overfitting and providing a more accurate\\r\\nestimate of the model's performance.\\nMissing data can be handled by \\ntechniques such as mean/median \\nimputation,\\rmode imputation, or \\nusing advanced methods like \\nmultiple imputation or K-Nearest\\r\\nNeighbors imputation.\\nQ.57\\nQ.58\\nQ.59\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 19, 'page_label': '20', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nExplain the purpose of the term 'ensemble learning' \\nand its benefits in model building.\\nWhat is the difference between unsupervised and \\nsupervised machine learning\\ralgorithms?\\nCan you explain the concept of 'clustering' and \\nprovide an example of when it is used?\\nWhat is the purpose of 'dimensionality reduction' in \\ndata analysis, and how is it\\rachieved?\\nEnsemble learning involves combining multiple models \\nto improve predictive\\rperformance and reduce \\noverfitting, often resulting in better generalisation and \\nmore robust\\rpredictions.\\r\\nSupervised learning uses labelled data for training, \\nwhile unsupervised learning\\rworks with unlabeled data \\nto find patterns and relationships.\\nClustering is an unsupervised learning technique used \\nto group similar data points\\rtogether. An example is \\ncustomer segmentation in marketing.\\nDimensionality reduction is used to reduce the number \\nof features in a dataset. It is\\rachieved through\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 19, 'page_label': '20', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='to group similar data points\\rtogether. An example is \\ncustomer segmentation in marketing.\\nDimensionality reduction is used to reduce the number \\nof features in a dataset. It is\\rachieved through \\ntechniques like principal component analysis (PCA) \\nand t-distributed\\rstochastic neighbour embedding (t-\\nSNE).\\r\\nQ.60\\nQ.61\\nQ.62\\nQ.63'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 20, 'page_label': '21', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nHow do you handle the problem of overfitting in \\nmachine learning models?\\nHow do you handle the problem of multicollinearity \\nin a dataset?\\nExplain the purpose of the term 'Naive Bayes' in \\nmachine learning and its application.\\nWhat is the purpose of the term 'decision trees' in \\nmachine learning, and how does it\\rwork?\\nOverfitting can be mitigated by using techniques like \\ncross-validation,\\rregularisation, early stopping, and \\nreducing model complexity.\\nMulticollinearity can be addressed by techniques such \\nas removing one of the\\rcorrelated features, using \\nprincipal component analysis (PCA), or using \\nregularisation\\rmethods.\\nNaive Bayes is a probabilistic classification algorithm \\nbased on Bayes' theorem\\rwith an assumption of \\nindependence between features. It is commonly used \\nfor text\\rclassification and spam filtering.\\nDecision trees are predictive models that map features \\nto conclusions about the\\rtarget value. They work by\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 20, 'page_label': '21', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='for text\\rclassification and spam filtering.\\nDecision trees are predictive models that map features \\nto conclusions about the\\rtarget value. They work by \\nsplitting the dataset into smaller subsets based on the \\nmost\\rsignificant differentiators in the data.\\nQ.64\\nQ.67\\nQ.65\\nQ.66'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 21, 'page_label': '22', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nCan you explain the purpose of the term 'random \\nforest' in machine learning and its\\radvantages?\\nWhat is the purpose of 'data preprocessing' in \\nmachine learning, and what are some\\rcommon \\ntechniques used?\\nHow do you handle the problem of underfitting in a \\nmachine learning model?\\nRandom forests are an ensemble learning method that \\nconstructs multiple decision\\rtrees during training. They \\nare effective for reducing overfitting and handling large \\ndatasets\\rwith high dimensionality.\\nData preprocessing involves preparing and cleaning \\ndata before it is fed into a\\rmachine learning model. \\nCommon techniques include data normalisation, \\nstandardisation,\\rand handling missing values.\\nUnderfitting can be addressed by using more complex \\nmodels, adding more\\rfeatures, or reducing \\nregularisation, allowing the model to capture more \\ncomplex patterns in\\rthe data.\\nQ.68\\nQ.69\\nQ.70\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 22, 'page_label': '23', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'hyperparameter tuning' in \\nmachine learning algorithms.\\nWhat is the purpose of 'ANOVA' (Analysis of \\nVariance) in statistical analysis, and when is\\rit used?\\nHow do you handle a situation where the data has \\noutliers?\\nExplain the concept of 'bias' in machine learning \\nmodels.\\nHyperparameter tuning involves finding the best set of \\nhyperparameters for a\\rmachine learning model to \\noptimise its performance and generalisation.\\nANOVA is used to analyse the differences among group \\nmeans and is applied\\rwhen comparing means of more \\nthan two groups to determine whether they are \\nstatistically\\rsignificantly different.\\nOutliers can be handled by removing them if they are \\ndue to data entry errors or by\\rtransforming them using \\ntechniques such as winsorization or log transformation.\\nBias refers to the error introduced by approximating a \\nreal-world problem, often\\rdue to oversimplification of \\nthe model. High bias can lead to underfitting.\\nQ.71\\nQ.72\\nQ.73\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 22, 'page_label': '23', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='Bias refers to the error introduced by approximating a \\nreal-world problem, often\\rdue to oversimplification of \\nthe model. High bias can lead to underfitting.\\nQ.71\\nQ.72\\nQ.73\\nQ.74'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 23, 'page_label': '24', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nWhat is the purpose of the 'mean squared error' \\nmetric in regression analysis?\\nCan you explain the purpose of the term 'cosine \\nsimilarity' in similarity measurements?\\nHow do you handle a situation where the data has \\na time component?\\nMean squared error is a commonly used metric for \\nevaluating the performance of a\\rregression model by \\nmeasuring the average of the squares of the \\ndifferences between\\rpredicted and actual values.\\nCosine similarity is a metric used to measure the \\nsimilarity between two non-zero\\rvectors, often used in \\ntext mining and collaborative filtering.\\nData with a time component can be analysed using \\ntime series analysis techniques\\rsuch as\\rautoregressive \\nintegrated moving average (ARIMA) models, \\nexponential smoothing, or\\rProphet forecasting models.\\r\\nQ.75\\nQ.76\\nQ.77\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 24, 'page_label': '25', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'precision' and 'recall' in the \\ncontext of classification models.\\nWhat is the purpose of the 'Hadoop' framework in \\nbig data processing, and how is it\\rused?\\nHow do you handle a situation where the data has \\na lot of noise?\\nPrecision measures the proportion of true positive \\nresults among the predicted\\rpositive results, while \\nrecall measures the proportion of true positive results \\namong the\\ractual positive results.\\nHadoop is an open-source framework used for \\ndistributed storage and processing\\rof large data sets \\nacross clusters of computers using simple \\nprogramming models.\\nNoisy data can be managed through techniques such \\nas data smoothing, filtering,\\ror by using robust \\nstatistical measures that are less sensitive to outliers.\\nQ.78\\nQ.79\\nQ.80\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 25, 'page_label': '26', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'correlation' in statistics and \\nits different types.\\nWhat is the purpose of the 'k-nearest neighbours' \\nalgorithm in machine learning, and\\rhow does it \\nwork?\\nHow do you handle a situation where the data has \\na lot of categorical variables?\\nCorrelation measures the relationship between two \\nvariables and can be positive,\\rnegative, or zero, \\nindicating the strength and direction of the \\nrelationship.\\r\\nThe k-nearest neighbours algorithm is used for \\nclassification and regression tasks,\\rmaking predictions \\nbased on the majority vote or averaging the values of \\nthe k nearest\\rneighbours.\\nCategorical variables can be handled through \\ntechniques such as one-hot\\rencoding, label encoding, \\nor using target encoding to convert them into a format \\nsuitable for\\rmachine learning models.\\nQ.81\\nQ.82\\nQ.83\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 26, 'page_label': '27', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"t u t o r t  a c a d e m yC u r a t e d  b y\\nExplain the purpose of the 'SVM' (Support Vector \\nMachine) algorithm in machine\\rlearning, and its \\nadvantages.\\nSupport Vector Machines are supervised learning \\nmodels used for classification\\rand regression analysis. \\nThey are effective in high-dimensional spaces and \\nwork well with\\rcomplex datasets.\\nQ.84\\nWhat is the purpose of the 'LSTM' \\n(Long Short-Term Memory) network in deep \\nlearning,\\rand how is it used?\\nCan you explain the purpose of the term 'Principal \\nComponent Analysis' (PCA)\\rin dimensionality \\nreduction, and how is it used?\\nLSTM networks are a type of recurrent neural network \\n(RNN) used for processing\\rand making predictions \\nbased on sequential data, often used in natural \\nlanguage processing\\rand time series analysis.\\nPrincipal Component Analysis is a technique used to \\nreduce the dimensionality of a\\rdataset while \\npreserving as much variance as possible. It transforms \\nthe original variables\\rinto a new set of variables, the\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 26, 'page_label': '27', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='reduce the dimensionality of a\\rdataset while \\npreserving as much variance as possible. It transforms \\nthe original variables\\rinto a new set of variables, the \\nprincipal components, which are orthogonal and\\r\\nuncorrelated. This aids in simplifying the dataset and \\nspeeding up the subsequent learning\\ralgorithms while \\nretaining most of the essential information.\\nQ.85\\nQ.86'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 27, 'page_label': '28', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'k-means clustering' and its \\napplication in unsupervised learning.\\nWhat is the purpose of the 'R-squared' metric in \\nregression analysis, and what does it\\rindicate \\nabout the model's fit?\\nWhat is the purpose of the term 't-Distributed \\nStochastic Neighbour Embedding' (t-SNE)\\r\\nin dimensionality reduction, and how is it used?\\nK-means clustering is a popular unsupervised learning \\nalgorithm used for\\rpartitioning a dataset into K clusters \\nbased on similarities in the data points.\\nR-squared is a statistical measure that represents the \\nproportion of the variance for\\ra dependent variable \\nexplained by the independent variables in a regression \\nmodel. It\\rindicates the goodness of fit of the model.\\nt-Distributed Stochastic Neighbour Embedding is a \\nnonlinear dimensionality\\rreduction technique used for \\nvisualising high-dimensional data in a low-\\ndimensional space. It\\ris particularly useful for \\nvisualising complex datasets and identifying patterns\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 27, 'page_label': '28', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='visualising high-dimensional data in a low-\\ndimensional space. It\\ris particularly useful for \\nvisualising complex datasets and identifying patterns \\nor clusters\\rwithin the data.\\nQ.87\\nQ.88\\nQ.89'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 28, 'page_label': '29', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nExplain the purpose of the 'F1 score' metric in \\nevaluating classification models and its\\r\\nrelationship with precision and recall.\\nCan you explain the concept of 'backpropagation' \\nin neural networks and its role in\\rtraining the \\nmodel?\\nThe F1 score is the harmonic mean of precision and \\nrecall and is used to evaluate\\rthe balance between \\nprecision and recall in a classification model.\\r\\nBackpropagation is an algorithm used to train artificial \\nneural networks by adjusting\\rthe weights of the \\nconnections in the network to minimise the difference \\nbetween predicted\\rand actual outputs.\\nQ.90\\nQ.91\\nWhat is the purpose of the 'chi-square test' in \\nstatistics, and when is it used?\\nThe chi-square test is used to determine the \\nindependence of two categorical\\rvariables and is often \\nused to test the significance of relationships between \\nvariables in a\\rcontingency table.\\r\\nQ.92\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 29, 'page_label': '30', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nHow do you handle a situation where the data is \\nnot normally distributed?\\nExplain the concept of 'latent variables' in the \\ncontext of factor analysis and its\\rimportance.\\nWhat is the purpose of the 'Gini index' in decision \\ntrees, and how is it used in the context\\rof building \\nthe tree?\\nNon-normally distributed data can be transformed \\nusing techniques such as the\\rBox-Cox transformation, \\nYeo-Johnson transformation, or log transformation to \\napproximate a\\rnormal distribution.\\nLatent variables are variables that are not directly \\nobserved but are inferred from\\robserved variables. \\nThey are crucial for capturing underlying factors and \\nreducing the\\rdimensionality of the data.\\nThe Gini index is a metric used to measure the impurity \\nof a node in a decision\\rtree. It is used to find the best \\nsplit for creating a more accurate decision tree.\\nQ.93\\nQ.94\\nQ.95\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 30, 'page_label': '31', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"t u t o r t  a c a d e m yC u r a t e d  b y\\nHow do you handle a situation where the data has \\na lot of continuous variables?\\nExplain the purpose of 'association rules' in data \\nmining, and provide an example of its\\rapplication.\\nWhat is the purpose of the 'logistic function' in \\nlogistic regression, and how is it used for\\r\\nbinary classification?\\nContinuous variables can be handled through \\ntechniques such as scaling and\\rnormalisation to \\nensure that the variables are on a similar scale, \\npreventing certain features\\rfrom dominating the \\nlearning process.\\nAssociation rules are used to discover interesting \\nrelationships between variables\\rin large datasets. An \\nexample is market basket analysis used to identify \\nproducts frequently\\rpurchased together.\\nThe logistic function is used to model the probability of \\na binary outcome. It maps\\rany real-valued number to \\na value between 0 and 1, making it suitable for binary\\r\\nclassification tasks.\\nQ.96\\nQ.97\\nQ.98\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 31, 'page_label': '32', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nHow do you handle a situation where the data has \\na lot of missing values?\\nExplain the concept of 'bagging' and 'boosting' in \\nensemble learning, and provide an\\r\\nexample of when each technique is used.\\nData with missing values can be managed through \\ntechniques such as imputation,\\rusing algorithms like \\nK-Nearest Neighbours, decision trees, or employing \\nadvanced\\rtechniques like deep learning-based \\nimputation.\\nBagging involves training multiple models \\nindependently and combining their\\rpredictions, while \\nboosting trains models sequentially, giving more \\nweight to misclassified\\rdata points. Bagging is used for \\nreducing variance, while boosting is used for reducing \\nbias\\rin ensemble models.\\nQ.99\\nQ.100\\nHighest \\nCTC\\nHiring\\nPartners350+Career \\nTransitions1250+ 2.1CR\\nWhy Tutort Academy?\"),\n",
       " Document(metadata={'producer': 'iText 2.1.3 (by lowagie.com)', 'creator': 'PyPDF', 'creationdate': '2026-02-02T21:28:22+05:30', 'moddate': '2026-02-02T21:28:22+05:30', 'source': '../data/pdf/ANNEX.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'ANNEX.pdf', 'file_type': 'pdf'}, page_content='INCOME TAX PAN SERVICES UNIT\\n Managed by Protean eGov Technologies Limited (formerly NSDL e-Governance Infrastructure Limited) \\n4th floor, Sapphire Chambers, Baner Road, Baner, Pune 411045.\\n \\nRef.No.TIN/PAN/CR-/99005971236288402 Date: 19-JAN-2026\\nShri BALMIKI BIHARI \\nC/O-BHIMASEN BIHARI BARIMUNDEI\\nBAGALSAHI NIALI\\nCUTTACK Orissa 754004\\nDear Sir/Madam,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t Subject  : Discrepancy in the documents received for PAN application\\n1. This has reference to documents received in connection with your request for New PAN card and/or changes/correction in PAN data for PAN AJOPB9797Q made vide\\nacknowledgement no. 990059712362884. Following discrepancies are observed between details provided in application and details available with Income Tax Department\\n(ITD).\\n \\n2. Please submit the below specified documents (with details as per application) to clear the above mentioned discrepancy (ies).'),\n",
       " Document(metadata={'producer': 'iText 2.1.3 (by lowagie.com)', 'creator': 'PyPDF', 'creationdate': '2026-02-02T21:28:22+05:30', 'moddate': '2026-02-02T21:28:22+05:30', 'source': '../data/pdf/ANNEX.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'ANNEX.pdf', 'file_type': 'pdf'}, page_content='(ITD).\\n \\n2. Please submit the below specified documents (with details as per application) to clear the above mentioned discrepancy (ies).\\n \\n3. Please note your PAN application will be processed only on receipt of documents as explained above.\\n4. If we do not receive documents as mentioned above within 30 days, then your application will be filed and no further action will be taken.\\n5. Information relating to all PAN Services of ITD can be obtained by making a phone call to Aaykar Sampark Kendra (1800-180-1961) or TIN-Call Centre (020-27218080)\\nor from the website: www.incometaxindia.gov.in or https://www.protean-tinpan.com/index.html \\n(This being a computer-generated letter,no signature is required) Income Tax Department\\nCaution : Income Tax Department does not send e-mails regarding refunds and does not seek any taxpayer information like user name, password, details of ATM, bank accounts,'),\n",
       " Document(metadata={'producer': 'iText 2.1.3 (by lowagie.com)', 'creator': 'PyPDF', 'creationdate': '2026-02-02T21:28:22+05:30', 'moddate': '2026-02-02T21:28:22+05:30', 'source': '../data/pdf/ANNEX.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'ANNEX.pdf', 'file_type': 'pdf'}, page_content=\"Caution : Income Tax Department does not send e-mails regarding refunds and does not seek any taxpayer information like user name, password, details of ATM, bank accounts,\\ncredit cards, etc. Taxpayers are advised not to part with such information on the basis of emails.\\n \\nTo be sent to Protean along with documents RETURN-SLIP         \\nParticular As per Application As per ITD's Database Discrepancy in documents submitted\\nApplicant's Name BALMIKI  BIHARI BALKI  BIHARI Name change from BALKI  BIHARI to BALMIKI  BIHARI has not been\\nspecified in AADHAR Card issued by the Unique Identification\\nAuthority of India provided by you (Please refer list  1)\\nFather's Name BHIMASEN  BIHARI BHIM SENA  BIHARI Name change from BALKI  BIHARI to BALMIKI  BIHARI has not been\\nspecified in AADHAR Card issued by the Unique Identification\\nAuthority of India provided by you (Please refer list  2)\\nName\\n( any one of the following - List 1 )\\nMarriage Certificate or Marriage invitation Card or\"),\n",
       " Document(metadata={'producer': 'iText 2.1.3 (by lowagie.com)', 'creator': 'PyPDF', 'creationdate': '2026-02-02T21:28:22+05:30', 'moddate': '2026-02-02T21:28:22+05:30', 'source': '../data/pdf/ANNEX.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'ANNEX.pdf', 'file_type': 'pdf'}, page_content=\"Authority of India provided by you (Please refer list  2)\\nName\\n( any one of the following - List 1 )\\nMarriage Certificate or Marriage invitation Card or\\nPublication of name change in official gazatte or copy of\\npassport showing husband's name or certificate issued by\\ngazetted officer along with copy of office Identity proof of\\ngazetted officer.\\nPublication of name change in official gazatte or certificate\\nissued by a gazetted officer along with copy of office\\nidentity proof of gazetted officer.\\nFather's Name\\n( any one of the following - List 2 )\\nPassport\\nElector's photo identity card\\nDriving License\\nCertificate of identity signed by a Member of Parliament or\\na Member of Legislative Assembly or a Muncipal councilor\\nor a Gazetted Officer along with copy of office identity\\nproof of issuing officer.\\nAny of the below mentioned document in the name of\\nKarta of HUF\\nAadhaar Card issued by the Unique Identification\\nAuthority of India\\nArm's license\"),\n",
       " Document(metadata={'producer': 'iText 2.1.3 (by lowagie.com)', 'creator': 'PyPDF', 'creationdate': '2026-02-02T21:28:22+05:30', 'moddate': '2026-02-02T21:28:22+05:30', 'source': '../data/pdf/ANNEX.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'ANNEX.pdf', 'file_type': 'pdf'}, page_content=\"proof of issuing officer.\\nAny of the below mentioned document in the name of\\nKarta of HUF\\nAadhaar Card issued by the Unique Identification\\nAuthority of India\\nArm's license\\nPensioner card having photograph of the applicant\\nCentral Government Health Service Scheme Card\\nBank certificate in Original on letter head from the branch\\n(alongwith name and stamp of the issuing officer)\\ncontaining duly attested photograph and bank account\\nnumber of the applicant\\nEx-Servicemen Contributory Health Scheme photo card\\nPhoto identity card issued by the Central Government or\\nState Government or Public Sector Undertaking\\n ACKNOWLEDGEMENT NO.990059712362884\\nPlease indicate how you want your application to be processed by putting tick in  appropriate boxes.\\nA.  Reprint PAN card with ITD data, no change required: [ ]\\nB.  For following fields, details available with Income Tax Department is / are correct and should not be changed (ignore application data):\"),\n",
       " Document(metadata={'producer': 'iText 2.1.3 (by lowagie.com)', 'creator': 'PyPDF', 'creationdate': '2026-02-02T21:28:22+05:30', 'moddate': '2026-02-02T21:28:22+05:30', 'source': '../data/pdf/ANNEX.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'ANNEX.pdf', 'file_type': 'pdf'}, page_content=\"B.  For following fields, details available with Income Tax Department is / are correct and should not be changed (ignore application data):\\n      [ ] Name            [ ] Father's Name          [ ] DOB\\nC.  For following fields, details available with Income Tax Department is / are incorrect and should be changed:\\n      [ ] Name            [ ] Father's Name            [ ] DOB          \\t\\t \\t\\t (provide documents to support changes as described overleaf)\\nList of Documents attached: (1)_____________(2)___________(3)___________(4)____________\\nName of Applicant: Shri BALMIKI BIHARI\\nSignature of Applicant: _______________\")]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding And VectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pratik/Downloads/YTRAG/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1702.57it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x131341520>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x174b5eb10>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 0, 'page_label': '1', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='with These\\n100 Ques tions\\nData Scientist \\nInterview\\nAce your Next'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 1, 'page_label': '2', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nWhat is the role of a data scientist in an \\norganisation?\\nExplain the difference between supervised and \\nunsupervised learning.\\r\\nWhat is cross-validation, and why is it important?\\r\\nA data scientist is responsible for collecting, analysing, \\nand interpreting complex data to help organisations \\nmake informed decisions.\\nSupervised learning uses labelled data for training, \\nwhile unsupervised learning\\rworks with unlabeled \\ndata to find hidden patterns or relationships.\\r\\nCross-validation is a technique used to assess how \\nwell a model generalises to\\ran independent dataset. It \\nis important for evaluating a model's performance and \\npreventing\\roverfitting.\\r\\nQ.1\\nQ.2\\nQ.3\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 2, 'page_label': '3', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nCan you explain the steps involved in the \\ndata preprocessing process?\\r\\nWhat are some common algorithms used in \\nmachine learning?\\r\\nData preprocessing includes data cleaning, handling \\nmissing values, data\\rtransformation, normalisation, \\nand standardisation to prepare the data for analysis \\nand\\rmodelling.\\r\\nCommon machine learning algorithms include linear \\nregression, logistic\\rregression, decision trees, random \\nforests, support vector machines, and neural networks.\\nQ.4\\nQ.5\\nHow do you handle missing data in a dataset?\\nMissing data can be handled by either removing the \\nrows with missing values,\\rimputing the missing values \\nusing statistical techniques, or using advanced \\nimputation\\rmethods such as K-Nearest Neighbors.\\nQ.6'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 3, 'page_label': '4', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nWhat is the purpose of the K-Means clustering \\nalgorithm?\\r\\nHow do you assess the performance of a machine \\nlearning model?\\r\\nExplain the term 'bias' in the context of machine \\nlearning models.\\r\\nWhat is the importance of feature scaling in \\nmachine learning?\\nThe K-Means algorithm is used for partitioning a \\ndataset into K clusters, aiming to\\rminimise the sum of \\nsquares within each cluster.\\r\\nModel performance can be assessed using metrics \\nsuch as accuracy, precision,\\rrecall, F1 score, and the \\nROC curve for classification tasks, and metrics such as \\nmean\\rsquared error for regression tasks.\\nBias refers to the error introduced by approximating a \\nreal-world problem, often\\rdue to oversimplification of \\nthe model. High bias can result in underfitting.\\r\\n Feature scaling ensures that the features are at a \\nsimilar scale, preventing\\rcertain features from \\ndominating the learning process and helping the \\nalgorithm converge\\rfaster.\\nQ.7\\nQ.8\\nQ.9\\nQ.10\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 4, 'page_label': '5', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nCan you explain the concept of regularisation in \\nmachine learning?\\r\\nWhat is the difference between L1 and L2 \\nregularisation?\\nRegularisation is a technique used to prevent \\noverfitting by adding a penalty term\\rto the loss \\nfunction, discouraging complex models.\\nL1 regularisation adds the absolute value of the \\nmagnitude of coefficients as a\\rpenalty term, while L2 \\nregularisation adds the square of the magnitude of \\ncoefficients as a\\rpenalty term.\\r\\nQ.11\\nQ.12\\nWhat is the purpose of a confusion matrix in \\nclassification tasks?\\r\\nA confusion matrix is used to visualise the performance \\nof a classification model,\\rshowing the counts of true \\npositive, true negative, false positive, and false \\nnegative\\rpredictions.\\nQ.13'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 5, 'page_label': '6', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nHow do you handle multicollinearity in a dataset?\\nCan you explain the difference between precision \\nand recall?\\nWhat is the purpose of the Naive Bayes algorithm \\nin machine learning?\\nMulticollinearity  can be handled by \\ntechniques such as removing one of \\nthe\\rcorrelated features, using principal \\ncomponent analysis, or using \\nregularisation techniques to\\rreduce \\nthe impact of correlated features.\\nPrecision refers to the ratio of correctly predicted \\npositive observations to the\\rtotal predicted positive \\nobservations, while recall refers to the ratio of correctly \\npredicted\\rpositive observations to the total actual \\npositive observations.\\nThe Naive Bayes algorithm is used for classification \\ntasks, based on the Bayes\\rtheorem with the \\nassumption of independence between features.\\nQ.14\\nQ.15\\nQ.16'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 6, 'page_label': '7', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nHow do you handle outliers in a dataset?\\nExplain the concept of the Central Limit Theorem.\\nWhat is the purpose of a decision tree algorithm in \\nmachine learning?\\nCan you explain the concept of ensemble \\nlearning?\\nOutliers can be handled by either removing them if \\nthey are due to data entry\\rerrors, or by transforming \\nthem using techniques such as winsorization or log \\ntransformation.\\nThe Central Limit Theorem states that the sampling \\ndistribution of the sample\\rmeans approaches a normal \\ndistribution as the sample size increases, regardless of \\nthe\\rshape of the population distribution.\\nDecision trees are used for both classification and \\nregression tasks, creating a\\rmodel that predicts the \\nvalue of a target variable by learning simple decision \\nrules inferred\\rfrom the data features.\\nEnsemble learning involves combining multiple \\nindividual models to improve the\\roverall performance \\nand predictive power of the learning algorithm.\\nQ.17\\nQ.18\\nQ.19\\nQ.20'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 7, 'page_label': '8', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the difference between bagging and \\nboosting?\\nExplain the purpose of the Random Forest \\nalgorithm in machine learning.\\nHow do you select the optimal number of clusters \\nin a K-Means clustering algorithm?\\nBagging involves training each model in the ensemble \\nwith a subset of the data,\\rwhile boosting focuses on \\ntraining each model sequentially, giving more weight \\nto the\\rmisclassified data points.\\nRandom Forest is an ensemble learning method that \\nconstructs multiple decision\\rtrees during training and \\noutputs the mode of the classes or the mean \\nprediction of the\\rindividual trees for classification and \\nregression tasks, respectively.\\nThe optimal number of clusters can be determined \\nusing techniques such as the\\relbow method, silhouette \\nscore, or the gap statistic.\\nQ.21\\nQ.22\\nQ.23'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 8, 'page_label': '9', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Support Vector Machine \\n(SVM) algorithm?\\nHow do you handle a large volume of data that \\ncannot fit into memory?\\nCan you explain the purpose of a recommendation \\nsystem?\\nWhat is the purpose of Principal Component \\nAnalysis (PCA) in machine learning?\\nSupport Vector Machines are used for classification \\nand regression analysis, with\\rthe primary goal of \\nfinding the hyperplane that best separates the classes.\\nLarge volumes of data can be handled using \\ntechniques such as data streaming,\\rdistributed \\ncomputing frameworks like Hadoop or Spark, and \\ndata compression techniques.\\nRecommendation systems are used to predict and \\nrecommend items or products\\rthat a user may be \\ninterested in, based on their past preferences or \\nbehaviour.\\nPrincipal Component Analysis is used for dimensionality \\nreduction, transforming\\ra large set of variables into a \\nsmaller set of uncorrelated variables while retaining \\nmost of the\\rinformation.\\nQ.24\\nQ.25\\nQ.26\\nQ.27'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 9, 'page_label': '10', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nHow do you handle a situation where the data is \\ntoo imbalanced?\\nWhat is the purpose of a Recurrent Neural Network \\n(RNN) in deep learning?\\nExplain the concept of a Long Short-Term Memory \\n(LSTM) network.\\nImbalanced data can be handled using techniques \\nsuch as oversampling the\\rminority class, \\nundersampling the majority class, or using algorithms \\nspecifically designed to\\rhandle imbalanced datasets.\\nRecurrent Neural Networks are used for sequence data, \\nallowing information to\\rpersist over time, making them \\nsuitable for tasks such as natural language processing \\nand\\rtime series analysis.\\nLSTM networks are a type of RNN that addresses the \\nvanishing gradient\\rproblem, making them more \\neffective for learning and predicting sequences of data.\\nQ.28\\nQ.29\\nQ.30'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 10, 'page_label': '11', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Word2Vec algorithm in \\nnatural language processing?\\nHow do you handle a situation where there are \\ntoo many features compared to the\\rnumber of \\nobservations?\\nExplain the concept of a support vector in the \\ncontext of a Support Vector Machine\\ralgorithm.\\nWord2Vec is used for learning word embeddings, \\nrepresenting words as vectors\\rto capture semantic \\nrelationships between words in a text corpus.\\nThe situation of having too many features compared \\nto the number of\\robservations can be handled by \\nusing feature selection techniques, such as Lasso\\r\\nregression, or by using dimensionality reduction \\ntechniques like PCA or t-SNE.\\r\\nSupport vectors are data points that lie closest to the \\ndecision boundary between\\rthe classes, influencing the \\nposition and orientation of the hyperplane in a Support \\nVector\\rMachine.\\nQ.31\\nQ.32\\nQ.33'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 11, 'page_label': '12', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Root Mean Square Error \\n(RMSE) metric in regression tasks?\\nCan you explain the purpose of the Apriori \\nalgorithm in association rule mining?\\nHow do you handle a situation where the data is \\nhighly skewed?\\nThe Root Mean Square Error is a commonly used metric \\nfor evaluating the\\raccuracy of a regression model by \\nmeasuring the differences between the predicted \\nvalues\\rand the actual values.\\nThe Apriori algorithm is used for discovering frequent \\nitemsets within a\\rtransactional database and is \\ncommonly employed in market basket analysis to \\nidentify\\rpatterns or relationships between different \\nitems.\\nHighly skewed data can be handled by using \\ntransformations such as log\\rtransformations, square \\nroot transformations, or by using specialised models \\nthat can handle\\rskewed data more effectively.\\nQ.34\\nQ.35\\nQ.36'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 12, 'page_label': '13', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Mean Average Precision \\n(MAP) metric in evaluating\\rinformation retrieval \\nsystems?\\nExplain the purpose of the Euclidean distance \\nmetric in clustering tasks.\\nHow do you handle a situation where the data is \\nnot linearly separable?\\nMean Average Precision is used to evaluate the \\nperformance of information\\rretrieval systems, \\nmeasuring the average precision at each relevant \\ndocument retrieved\\racross multiple queries.\\nThe Euclidean distance metric is used to measure the \\ndistance between two\\rpoints in a multidimensional \\nspace and is commonly used in clustering algorithms \\nsuch as\\rK-Means.\\nIn cases where the data is not linearly separable, kernel \\nfunctions can be used in\\ralgorithms like Support Vector \\nMachines to map the data to a higher-dimensional \\nspace\\rwhere it becomes linearly separable.\\nQ.37\\nQ.38\\nQ.39'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 13, 'page_label': '14', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Chi-square test in \\nfeature selection?\\nCan you explain the purpose of the Gradient \\nDescent algorithm in machine learning?\\nHow do you handle a situation where the data is \\ntime-series data?\\nThe Chi-square test is used to determine the \\nindependence of two categorical\\rvariables, making it \\nsuitable for feature selection in classification tasks.\\nGradient Descent is an optimization algorithm used to \\nminimise the cost function\\rand find the optimal \\nparameters of a model by iteratively updating the \\nparameters in the\\rdirection of the steepest descent.\\nTime-series data can be handled using techniques \\nsuch as autoregressive\\rintegrated moving average \\n(ARIMA) models, exponential smoothing methods, or \\nmore\\radvanced deep learning models like Long Short-\\nTerm Memory (LSTM) networks.\\nQ.40\\nQ.41\\nQ.42'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 14, 'page_label': '15', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the K-Nearest Neighbors \\n(KNN) algorithm in machine learning?\\nExplain the purpose of the Log Loss metric in \\nevaluating classification models.\\nHow do you handle a situation where the data is \\nhigh-dimensional?\\nThe K-Nearest Neighbors algorithm is used for \\nclassification and regression\\rtasks, making predictions \\nbased on the majority vote of its k nearest neighbours.\\nLog Loss is used to evaluate the performance of a \\nclassification model that\\routputs probabilities, \\nmeasuring the performance based on the likelihood of \\nthe predicted\\rprobabilities matching the actual labels.\\nHigh-dimensional data can be handled by using \\ndimensionality reduction\\rtechniques such as Principal \\nComponent Analysis (PCA), t-Distributed Stochastic \\nNeighbour\\rEmbedding (t-SNE), or by employing feature \\nselection methods.\\nQ.43\\nQ.44\\nQ.45'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 15, 'page_label': '16', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the R-squared (R2) metric \\nin evaluating regression models?\\nCan you explain the purpose of the Gini index in \\nthe context of a decision tree\\ralgorithm?\\nHow do you handle a situation where there is noise \\nin the data?\\r\\nR-squared is a statistical measure that represents the \\nproportion of the variance\\rfor a dependent variable \\nthat is explained by an independent variable in a \\nregression model.\\nThe Gini index is used to measure the impurity or the \\nhomogeneity of a node in a\\rdecision tree, helping to \\ndetermine the best split for creating a more accurate \\ndecision tree.\\r\\nNoise in the data can be handled by smoothing \\ntechniques such as moving\\raverages, using robust \\nstatistics, or employing filtering methods to remove \\noutliers and\\rirrelevant data points.\\nQ.46\\nQ.47\\nQ.48'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 16, 'page_label': '17', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nWhat is the purpose of the F1 score metric in \\nevaluating classification models?\\nWhat is the difference between classification and \\nregression in machine learning?\\nCan you explain the bias-variance trade-off in the \\ncontext of model complexity?\\nCan you explain the purpose of the LDA \\n(Linear Discriminant Analysis) algorithm in\\r\\nmachine learning?\\nThe F1 score is the harmonic mean of precision and \\nrecall and is used to\\revaluate the balance between \\nprecision and recall in a classification model.\\nClassification is used to predict discrete categories, \\nwhile regression is used to\\rpredict continuous \\nquantities.\\r\\nThe bias-variance trade-off highlights the trade-off \\nbetween a model's ability to\\rminimise errors due to \\nbias and variance. Increasing model complexity \\nreduces bias but\\rincreases variance and vice versa.\\nLinear Discriminant Analysis is used for dimensionality \\nreduction and\\rclassification tasks, aiming to find the\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 16, 'page_label': '17', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='reduces bias but\\rincreases variance and vice versa.\\nLinear Discriminant Analysis is used for dimensionality \\nreduction and\\rclassification tasks, aiming to find the \\nlinear combinations of features that best separate\\r\\nmultiple classes in the data.\\nQ.49\\nQ.51\\nQ.52\\nQ.50'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 17, 'page_label': '18', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nHow do you handle imbalanced data sets when \\nbuilding a classification model?\\nExplain the purpose of the term 'regularisation' in \\nmachine learning models.\\nHow do you assess the performance of a \\nclassification model apart from accuracy?\\nWhat is the purpose of the term 'gradient descent' \\nin the context of optimising a model?\\nImbalanced datasets can be handled using techniques \\nlike oversampling,\\rundersampling, or using algorithms \\ndesigned for imbalanced data such as SMOTE\\r\\n(Synthetic Minority Over-sampling Technique).\\nRegularisation is a technique used to prevent \\noverfitting by adding a penalty term\\rto the loss \\nfunction, discouraging overly complex models.\\nThe performance of a classification model can be \\nevaluated using metrics such as\\rprecision, recall, F1 \\nscore, and the area under the ROC curve.\\nGradient descent is an iterative optimization algorithm \\nused to minimise the cost\\rfunction of a model by \\nadjusting the model's parameters in the direction of\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 17, 'page_label': '18', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"Gradient descent is an iterative optimization algorithm \\nused to minimise the cost\\rfunction of a model by \\nadjusting the model's parameters in the direction of \\nsteepest descent.\\r\\nQ.53\\nQ.54\\nQ.56\\nQ.55\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 18, 'page_label': '19', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nCan you explain the concept of 'feature selection' \\nand its importance in model building?\\nWhat is the purpose of the term 'cross-validation' \\nin model training and evaluation?\\nHow do you handle missing data in a dataset while \\nbuilding a predictive model?\\nFeature selection involves selecting the most relevant \\nfeatures from a dataset. It is\\rcrucial for improving \\nmodel performance, reducing overfitting, and \\nenhancing interpretability.\\nCross-validation is used to assess how well a model \\ngeneralises to an\\rindependent dataset, minimising the \\nrisk of overfitting and providing a more accurate\\r\\nestimate of the model's performance.\\nMissing data can be handled by \\ntechniques such as mean/median \\nimputation,\\rmode imputation, or \\nusing advanced methods like \\nmultiple imputation or K-Nearest\\r\\nNeighbors imputation.\\nQ.57\\nQ.58\\nQ.59\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 19, 'page_label': '20', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nExplain the purpose of the term 'ensemble learning' \\nand its benefits in model building.\\nWhat is the difference between unsupervised and \\nsupervised machine learning\\ralgorithms?\\nCan you explain the concept of 'clustering' and \\nprovide an example of when it is used?\\nWhat is the purpose of 'dimensionality reduction' in \\ndata analysis, and how is it\\rachieved?\\nEnsemble learning involves combining multiple models \\nto improve predictive\\rperformance and reduce \\noverfitting, often resulting in better generalisation and \\nmore robust\\rpredictions.\\r\\nSupervised learning uses labelled data for training, \\nwhile unsupervised learning\\rworks with unlabeled data \\nto find patterns and relationships.\\nClustering is an unsupervised learning technique used \\nto group similar data points\\rtogether. An example is \\ncustomer segmentation in marketing.\\nDimensionality reduction is used to reduce the number \\nof features in a dataset. It is\\rachieved through\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 19, 'page_label': '20', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='to group similar data points\\rtogether. An example is \\ncustomer segmentation in marketing.\\nDimensionality reduction is used to reduce the number \\nof features in a dataset. It is\\rachieved through \\ntechniques like principal component analysis (PCA) \\nand t-distributed\\rstochastic neighbour embedding (t-\\nSNE).\\r\\nQ.60\\nQ.61\\nQ.62\\nQ.63'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 20, 'page_label': '21', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nHow do you handle the problem of overfitting in \\nmachine learning models?\\nHow do you handle the problem of multicollinearity \\nin a dataset?\\nExplain the purpose of the term 'Naive Bayes' in \\nmachine learning and its application.\\nWhat is the purpose of the term 'decision trees' in \\nmachine learning, and how does it\\rwork?\\nOverfitting can be mitigated by using techniques like \\ncross-validation,\\rregularisation, early stopping, and \\nreducing model complexity.\\nMulticollinearity can be addressed by techniques such \\nas removing one of the\\rcorrelated features, using \\nprincipal component analysis (PCA), or using \\nregularisation\\rmethods.\\nNaive Bayes is a probabilistic classification algorithm \\nbased on Bayes' theorem\\rwith an assumption of \\nindependence between features. It is commonly used \\nfor text\\rclassification and spam filtering.\\nDecision trees are predictive models that map features \\nto conclusions about the\\rtarget value. They work by\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 20, 'page_label': '21', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='for text\\rclassification and spam filtering.\\nDecision trees are predictive models that map features \\nto conclusions about the\\rtarget value. They work by \\nsplitting the dataset into smaller subsets based on the \\nmost\\rsignificant differentiators in the data.\\nQ.64\\nQ.67\\nQ.65\\nQ.66'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 21, 'page_label': '22', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nCan you explain the purpose of the term 'random \\nforest' in machine learning and its\\radvantages?\\nWhat is the purpose of 'data preprocessing' in \\nmachine learning, and what are some\\rcommon \\ntechniques used?\\nHow do you handle the problem of underfitting in a \\nmachine learning model?\\nRandom forests are an ensemble learning method that \\nconstructs multiple decision\\rtrees during training. They \\nare effective for reducing overfitting and handling large \\ndatasets\\rwith high dimensionality.\\nData preprocessing involves preparing and cleaning \\ndata before it is fed into a\\rmachine learning model. \\nCommon techniques include data normalisation, \\nstandardisation,\\rand handling missing values.\\nUnderfitting can be addressed by using more complex \\nmodels, adding more\\rfeatures, or reducing \\nregularisation, allowing the model to capture more \\ncomplex patterns in\\rthe data.\\nQ.68\\nQ.69\\nQ.70\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 22, 'page_label': '23', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'hyperparameter tuning' in \\nmachine learning algorithms.\\nWhat is the purpose of 'ANOVA' (Analysis of \\nVariance) in statistical analysis, and when is\\rit used?\\nHow do you handle a situation where the data has \\noutliers?\\nExplain the concept of 'bias' in machine learning \\nmodels.\\nHyperparameter tuning involves finding the best set of \\nhyperparameters for a\\rmachine learning model to \\noptimise its performance and generalisation.\\nANOVA is used to analyse the differences among group \\nmeans and is applied\\rwhen comparing means of more \\nthan two groups to determine whether they are \\nstatistically\\rsignificantly different.\\nOutliers can be handled by removing them if they are \\ndue to data entry errors or by\\rtransforming them using \\ntechniques such as winsorization or log transformation.\\nBias refers to the error introduced by approximating a \\nreal-world problem, often\\rdue to oversimplification of \\nthe model. High bias can lead to underfitting.\\nQ.71\\nQ.72\\nQ.73\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 22, 'page_label': '23', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='Bias refers to the error introduced by approximating a \\nreal-world problem, often\\rdue to oversimplification of \\nthe model. High bias can lead to underfitting.\\nQ.71\\nQ.72\\nQ.73\\nQ.74'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 23, 'page_label': '24', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nWhat is the purpose of the 'mean squared error' \\nmetric in regression analysis?\\nCan you explain the purpose of the term 'cosine \\nsimilarity' in similarity measurements?\\nHow do you handle a situation where the data has \\na time component?\\nMean squared error is a commonly used metric for \\nevaluating the performance of a\\rregression model by \\nmeasuring the average of the squares of the \\ndifferences between\\rpredicted and actual values.\\nCosine similarity is a metric used to measure the \\nsimilarity between two non-zero\\rvectors, often used in \\ntext mining and collaborative filtering.\\nData with a time component can be analysed using \\ntime series analysis techniques\\rsuch as\\rautoregressive \\nintegrated moving average (ARIMA) models, \\nexponential smoothing, or\\rProphet forecasting models.\\r\\nQ.75\\nQ.76\\nQ.77\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 24, 'page_label': '25', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'precision' and 'recall' in the \\ncontext of classification models.\\nWhat is the purpose of the 'Hadoop' framework in \\nbig data processing, and how is it\\rused?\\nHow do you handle a situation where the data has \\na lot of noise?\\nPrecision measures the proportion of true positive \\nresults among the predicted\\rpositive results, while \\nrecall measures the proportion of true positive results \\namong the\\ractual positive results.\\nHadoop is an open-source framework used for \\ndistributed storage and processing\\rof large data sets \\nacross clusters of computers using simple \\nprogramming models.\\nNoisy data can be managed through techniques such \\nas data smoothing, filtering,\\ror by using robust \\nstatistical measures that are less sensitive to outliers.\\nQ.78\\nQ.79\\nQ.80\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 25, 'page_label': '26', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'correlation' in statistics and \\nits different types.\\nWhat is the purpose of the 'k-nearest neighbours' \\nalgorithm in machine learning, and\\rhow does it \\nwork?\\nHow do you handle a situation where the data has \\na lot of categorical variables?\\nCorrelation measures the relationship between two \\nvariables and can be positive,\\rnegative, or zero, \\nindicating the strength and direction of the \\nrelationship.\\r\\nThe k-nearest neighbours algorithm is used for \\nclassification and regression tasks,\\rmaking predictions \\nbased on the majority vote or averaging the values of \\nthe k nearest\\rneighbours.\\nCategorical variables can be handled through \\ntechniques such as one-hot\\rencoding, label encoding, \\nor using target encoding to convert them into a format \\nsuitable for\\rmachine learning models.\\nQ.81\\nQ.82\\nQ.83\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 26, 'page_label': '27', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"t u t o r t  a c a d e m yC u r a t e d  b y\\nExplain the purpose of the 'SVM' (Support Vector \\nMachine) algorithm in machine\\rlearning, and its \\nadvantages.\\nSupport Vector Machines are supervised learning \\nmodels used for classification\\rand regression analysis. \\nThey are effective in high-dimensional spaces and \\nwork well with\\rcomplex datasets.\\nQ.84\\nWhat is the purpose of the 'LSTM' \\n(Long Short-Term Memory) network in deep \\nlearning,\\rand how is it used?\\nCan you explain the purpose of the term 'Principal \\nComponent Analysis' (PCA)\\rin dimensionality \\nreduction, and how is it used?\\nLSTM networks are a type of recurrent neural network \\n(RNN) used for processing\\rand making predictions \\nbased on sequential data, often used in natural \\nlanguage processing\\rand time series analysis.\\nPrincipal Component Analysis is a technique used to \\nreduce the dimensionality of a\\rdataset while \\npreserving as much variance as possible. It transforms \\nthe original variables\\rinto a new set of variables, the\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 26, 'page_label': '27', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='reduce the dimensionality of a\\rdataset while \\npreserving as much variance as possible. It transforms \\nthe original variables\\rinto a new set of variables, the \\nprincipal components, which are orthogonal and\\r\\nuncorrelated. This aids in simplifying the dataset and \\nspeeding up the subsequent learning\\ralgorithms while \\nretaining most of the essential information.\\nQ.85\\nQ.86'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 27, 'page_label': '28', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'k-means clustering' and its \\napplication in unsupervised learning.\\nWhat is the purpose of the 'R-squared' metric in \\nregression analysis, and what does it\\rindicate \\nabout the model's fit?\\nWhat is the purpose of the term 't-Distributed \\nStochastic Neighbour Embedding' (t-SNE)\\r\\nin dimensionality reduction, and how is it used?\\nK-means clustering is a popular unsupervised learning \\nalgorithm used for\\rpartitioning a dataset into K clusters \\nbased on similarities in the data points.\\nR-squared is a statistical measure that represents the \\nproportion of the variance for\\ra dependent variable \\nexplained by the independent variables in a regression \\nmodel. It\\rindicates the goodness of fit of the model.\\nt-Distributed Stochastic Neighbour Embedding is a \\nnonlinear dimensionality\\rreduction technique used for \\nvisualising high-dimensional data in a low-\\ndimensional space. It\\ris particularly useful for \\nvisualising complex datasets and identifying patterns\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 27, 'page_label': '28', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content='visualising high-dimensional data in a low-\\ndimensional space. It\\ris particularly useful for \\nvisualising complex datasets and identifying patterns \\nor clusters\\rwithin the data.\\nQ.87\\nQ.88\\nQ.89'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 28, 'page_label': '29', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nExplain the purpose of the 'F1 score' metric in \\nevaluating classification models and its\\r\\nrelationship with precision and recall.\\nCan you explain the concept of 'backpropagation' \\nin neural networks and its role in\\rtraining the \\nmodel?\\nThe F1 score is the harmonic mean of precision and \\nrecall and is used to evaluate\\rthe balance between \\nprecision and recall in a classification model.\\r\\nBackpropagation is an algorithm used to train artificial \\nneural networks by adjusting\\rthe weights of the \\nconnections in the network to minimise the difference \\nbetween predicted\\rand actual outputs.\\nQ.90\\nQ.91\\nWhat is the purpose of the 'chi-square test' in \\nstatistics, and when is it used?\\nThe chi-square test is used to determine the \\nindependence of two categorical\\rvariables and is often \\nused to test the significance of relationships between \\nvariables in a\\rcontingency table.\\r\\nQ.92\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 29, 'page_label': '30', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nHow do you handle a situation where the data is \\nnot normally distributed?\\nExplain the concept of 'latent variables' in the \\ncontext of factor analysis and its\\rimportance.\\nWhat is the purpose of the 'Gini index' in decision \\ntrees, and how is it used in the context\\rof building \\nthe tree?\\nNon-normally distributed data can be transformed \\nusing techniques such as the\\rBox-Cox transformation, \\nYeo-Johnson transformation, or log transformation to \\napproximate a\\rnormal distribution.\\nLatent variables are variables that are not directly \\nobserved but are inferred from\\robserved variables. \\nThey are crucial for capturing underlying factors and \\nreducing the\\rdimensionality of the data.\\nThe Gini index is a metric used to measure the impurity \\nof a node in a decision\\rtree. It is used to find the best \\nsplit for creating a more accurate decision tree.\\nQ.93\\nQ.94\\nQ.95\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 30, 'page_label': '31', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"t u t o r t  a c a d e m yC u r a t e d  b y\\nHow do you handle a situation where the data has \\na lot of continuous variables?\\nExplain the purpose of 'association rules' in data \\nmining, and provide an example of its\\rapplication.\\nWhat is the purpose of the 'logistic function' in \\nlogistic regression, and how is it used for\\r\\nbinary classification?\\nContinuous variables can be handled through \\ntechniques such as scaling and\\rnormalisation to \\nensure that the variables are on a similar scale, \\npreventing certain features\\rfrom dominating the \\nlearning process.\\nAssociation rules are used to discover interesting \\nrelationships between variables\\rin large datasets. An \\nexample is market basket analysis used to identify \\nproducts frequently\\rpurchased together.\\nThe logistic function is used to model the probability of \\na binary outcome. It maps\\rany real-valued number to \\na value between 0 and 1, making it suitable for binary\\r\\nclassification tasks.\\nQ.96\\nQ.97\\nQ.98\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '../data/pdf/Data_science_Interview_question_2.pdf', 'total_pages': 32, 'page': 31, 'page_label': '32', 'source_file': 'Data_science_Interview_question_2.pdf', 'file_type': 'pdf'}, page_content=\"C u r a t e d  b y\\nHow do you handle a situation where the data has \\na lot of missing values?\\nExplain the concept of 'bagging' and 'boosting' in \\nensemble learning, and provide an\\r\\nexample of when each technique is used.\\nData with missing values can be managed through \\ntechniques such as imputation,\\rusing algorithms like \\nK-Nearest Neighbours, decision trees, or employing \\nadvanced\\rtechniques like deep learning-based \\nimputation.\\nBagging involves training multiple models \\nindependently and combining their\\rpredictions, while \\nboosting trains models sequentially, giving more \\nweight to misclassified\\rdata points. Bagging is used for \\nreducing variance, while boosting is used for reducing \\nbias\\rin ensemble models.\\nQ.99\\nQ.100\\nHighest \\nCTC\\nHiring\\nPartners350+Career \\nTransitions1250+ 2.1CR\\nWhy Tutort Academy?\"),\n",
       " Document(metadata={'producer': 'iText 2.1.3 (by lowagie.com)', 'creator': 'PyPDF', 'creationdate': '2026-02-02T21:28:22+05:30', 'moddate': '2026-02-02T21:28:22+05:30', 'source': '../data/pdf/ANNEX.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'ANNEX.pdf', 'file_type': 'pdf'}, page_content='INCOME TAX PAN SERVICES UNIT\\n Managed by Protean eGov Technologies Limited (formerly NSDL e-Governance Infrastructure Limited) \\n4th floor, Sapphire Chambers, Baner Road, Baner, Pune 411045.\\n \\nRef.No.TIN/PAN/CR-/99005971236288402 Date: 19-JAN-2026\\nShri BALMIKI BIHARI \\nC/O-BHIMASEN BIHARI BARIMUNDEI\\nBAGALSAHI NIALI\\nCUTTACK Orissa 754004\\nDear Sir/Madam,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t Subject  : Discrepancy in the documents received for PAN application\\n1. This has reference to documents received in connection with your request for New PAN card and/or changes/correction in PAN data for PAN AJOPB9797Q made vide\\nacknowledgement no. 990059712362884. Following discrepancies are observed between details provided in application and details available with Income Tax Department\\n(ITD).\\n \\n2. Please submit the below specified documents (with details as per application) to clear the above mentioned discrepancy (ies).'),\n",
       " Document(metadata={'producer': 'iText 2.1.3 (by lowagie.com)', 'creator': 'PyPDF', 'creationdate': '2026-02-02T21:28:22+05:30', 'moddate': '2026-02-02T21:28:22+05:30', 'source': '../data/pdf/ANNEX.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'ANNEX.pdf', 'file_type': 'pdf'}, page_content='(ITD).\\n \\n2. Please submit the below specified documents (with details as per application) to clear the above mentioned discrepancy (ies).\\n \\n3. Please note your PAN application will be processed only on receipt of documents as explained above.\\n4. If we do not receive documents as mentioned above within 30 days, then your application will be filed and no further action will be taken.\\n5. Information relating to all PAN Services of ITD can be obtained by making a phone call to Aaykar Sampark Kendra (1800-180-1961) or TIN-Call Centre (020-27218080)\\nor from the website: www.incometaxindia.gov.in or https://www.protean-tinpan.com/index.html \\n(This being a computer-generated letter,no signature is required) Income Tax Department\\nCaution : Income Tax Department does not send e-mails regarding refunds and does not seek any taxpayer information like user name, password, details of ATM, bank accounts,'),\n",
       " Document(metadata={'producer': 'iText 2.1.3 (by lowagie.com)', 'creator': 'PyPDF', 'creationdate': '2026-02-02T21:28:22+05:30', 'moddate': '2026-02-02T21:28:22+05:30', 'source': '../data/pdf/ANNEX.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'ANNEX.pdf', 'file_type': 'pdf'}, page_content=\"Caution : Income Tax Department does not send e-mails regarding refunds and does not seek any taxpayer information like user name, password, details of ATM, bank accounts,\\ncredit cards, etc. Taxpayers are advised not to part with such information on the basis of emails.\\n \\nTo be sent to Protean along with documents RETURN-SLIP         \\nParticular As per Application As per ITD's Database Discrepancy in documents submitted\\nApplicant's Name BALMIKI  BIHARI BALKI  BIHARI Name change from BALKI  BIHARI to BALMIKI  BIHARI has not been\\nspecified in AADHAR Card issued by the Unique Identification\\nAuthority of India provided by you (Please refer list  1)\\nFather's Name BHIMASEN  BIHARI BHIM SENA  BIHARI Name change from BALKI  BIHARI to BALMIKI  BIHARI has not been\\nspecified in AADHAR Card issued by the Unique Identification\\nAuthority of India provided by you (Please refer list  2)\\nName\\n( any one of the following - List 1 )\\nMarriage Certificate or Marriage invitation Card or\"),\n",
       " Document(metadata={'producer': 'iText 2.1.3 (by lowagie.com)', 'creator': 'PyPDF', 'creationdate': '2026-02-02T21:28:22+05:30', 'moddate': '2026-02-02T21:28:22+05:30', 'source': '../data/pdf/ANNEX.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'ANNEX.pdf', 'file_type': 'pdf'}, page_content=\"Authority of India provided by you (Please refer list  2)\\nName\\n( any one of the following - List 1 )\\nMarriage Certificate or Marriage invitation Card or\\nPublication of name change in official gazatte or copy of\\npassport showing husband's name or certificate issued by\\ngazetted officer along with copy of office Identity proof of\\ngazetted officer.\\nPublication of name change in official gazatte or certificate\\nissued by a gazetted officer along with copy of office\\nidentity proof of gazetted officer.\\nFather's Name\\n( any one of the following - List 2 )\\nPassport\\nElector's photo identity card\\nDriving License\\nCertificate of identity signed by a Member of Parliament or\\na Member of Legislative Assembly or a Muncipal councilor\\nor a Gazetted Officer along with copy of office identity\\nproof of issuing officer.\\nAny of the below mentioned document in the name of\\nKarta of HUF\\nAadhaar Card issued by the Unique Identification\\nAuthority of India\\nArm's license\"),\n",
       " Document(metadata={'producer': 'iText 2.1.3 (by lowagie.com)', 'creator': 'PyPDF', 'creationdate': '2026-02-02T21:28:22+05:30', 'moddate': '2026-02-02T21:28:22+05:30', 'source': '../data/pdf/ANNEX.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'ANNEX.pdf', 'file_type': 'pdf'}, page_content=\"proof of issuing officer.\\nAny of the below mentioned document in the name of\\nKarta of HUF\\nAadhaar Card issued by the Unique Identification\\nAuthority of India\\nArm's license\\nPensioner card having photograph of the applicant\\nCentral Government Health Service Scheme Card\\nBank certificate in Original on letter head from the branch\\n(alongwith name and stamp of the issuing officer)\\ncontaining duly attested photograph and bank account\\nnumber of the applicant\\nEx-Servicemen Contributory Health Scheme photo card\\nPhoto identity card issued by the Central Government or\\nState Government or Public Sector Undertaking\\n ACKNOWLEDGEMENT NO.990059712362884\\nPlease indicate how you want your application to be processed by putting tick in  appropriate boxes.\\nA.  Reprint PAN card with ITD data, no change required: [ ]\\nB.  For following fields, details available with Income Tax Department is / are correct and should not be changed (ignore application data):\"),\n",
       " Document(metadata={'producer': 'iText 2.1.3 (by lowagie.com)', 'creator': 'PyPDF', 'creationdate': '2026-02-02T21:28:22+05:30', 'moddate': '2026-02-02T21:28:22+05:30', 'source': '../data/pdf/ANNEX.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'ANNEX.pdf', 'file_type': 'pdf'}, page_content=\"B.  For following fields, details available with Income Tax Department is / are correct and should not be changed (ignore application data):\\n      [ ] Name            [ ] Father's Name          [ ] DOB\\nC.  For following fields, details available with Income Tax Department is / are incorrect and should be changed:\\n      [ ] Name            [ ] Father's Name            [ ] DOB          \\t\\t \\t\\t (provide documents to support changes as described overleaf)\\nList of Documents attached: (1)_____________(2)___________(3)___________(4)____________\\nName of Applicant: Shri BALMIKI BIHARI\\nSignature of Applicant: _______________\")]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 45 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (45, 384)\n",
      "Adding 45 documents to vector store...\n",
      "Successfully added 45 documents to vector store\n",
      "Total documents in collection: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##store int he vector dtaabase\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriver Pipeline From VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x1769cd400>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is attention is all you need'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is attention is all you need\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YTRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
